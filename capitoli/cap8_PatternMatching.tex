\chapter{Pattern Matching}
\label{cap:PatternMatching}



Introduciamo di seguito la terminologia di base:
\begin{itemize}
    \item \(\Sigma\) : l'alfabeto, ovvero l'insieme di caratteri possibili.
    \item \(|\Sigma|\) : la dimensione dell'alfabeto.
    \item Stringa \( S \) : una sequenza finita di caratteri appartenenti all'alfabeto \(\Sigma\), di lunghezza \( m \).
    \item \(S[i]\) : il carattere alla posizione \( i \) della stringa \( S \).
        \begin{itemize}[nosep]
            \item In Python: $S$[i] 
        \end{itemize}
    \item \(S[i..j]\) : la sottostringa di \( S \) che va dall'indice \( i \) all'indice \( j \).
        \begin{itemize}[nosep]
            \item In Python: $S$[i:j+1] 
        \end{itemize}
    \item \(S[0..k]\) : prefisso di lunghezza \( k+1 \) della stringa \( S \).
        \begin{itemize}[nosep]
            \item In Python: $S$[:k+1]
        \end{itemize}
    \item \(S[j..m-1]\) : suffisso di lunghezza \( m-j \) della stringa \( S \). 
        \begin{itemize}[nosep]
            \item In Python: $S$[j:]
        \end{itemize}
\end{itemize}

\noindent
Convenzioni Python: 
\begin{itemize}[nosep]
    \item La sottostringa $S$[j:j+m] ha lunghezza $m$.
    \item La sottostringa $S$[j:j] è la stringa nulla, di lunghezza 0.
    \item La sottostringa $S$[j:k] è la stringa nulla quando $k < j$.
\end{itemize}


\vspace{1\baselineskip}
\noindent
Nel classico problema di pattern matching, ci viene data una stringa di testo $T$ di lunghezza $n$ e una stringa di pattern $P$ di lunghezza $m$, e vogliamo scoprire se $P$ è una sottostringa di $T$. In tal caso, potremmo voler trovare l'indice più basso $j$ all'interno di $T$ in cui inizia $P$, in modo che $T[j..j+m-1]$ sia uguale a $P$, o forse trovare tutti gli indici di $T$ in cui inizia il pattern $P$.

\clearpage
\section{Brute Force}
Il metodo più semplice per risolvere il problema del pattern matching è il metodo \textit{brute force}. L'idea alla base di questo metodo è di confrontare il pattern $P$ con ogni possibile sottostringa di $T$ di lunghezza $m$. In particolare, per ogni indice $i$ da $0$ a $n-m$, confrontiamo la sottostringa $T[i..i+m-1]$ con il pattern $P$. Se troviamo una corrispondenza, restituiamo l'indice $i$.

\vspace{1\baselineskip}
\begin{lstlisting}
def find_brute(T, P):
    """Return the lowest index of T at which substring P begins (or else -1)."""
    n, m = len(T), len(P)  # introduce convenient notations
    for i in range(n-m+1):  # try every potential starting index within T
        k = 0  # an index into pattern P
        while k < m and T[i + k] == P[k]:  # kth character of P matches
            k += 1
        if k == m:  # if we reached the end of pattern,
            return i  # substring T[i:i+m] matches P
    return -1  # failed to find a match starting with any i
\end{lstlisting}
\vspace{1\baselineskip}


\subsection*{Performance}
L'algoritmo consiste in due cicli annidati, con il ciclo esterno che scorre tutti i possibili indici iniziali del pattern nel testo $T$, e il ciclo interno che scorre ogni carattere del pattern $P$, confrontandolo con il suo potenziale carattere corrispondente nel testo. Pertanto, la correttezza dell'algoritmo deriva direttamente da questo approccio di ricerca esaustiva.

Il tempo di esecuzione del pattern matching tramite \emph{Brute Force} nel caso peggiore non è buono poiché per ogni indice candidato in $T$, possiamo eseguire fino a $m$ confronti di caratteri per scoprire che $P$ non corrisponde a $T$ all'indice corrente. Dal blocco di codice si può osservare che il ciclo $for$ esterno viene eseguito al massimo $n-m+1$ volte e il ciclo $while$ interno viene eseguito al massimo $m$ volte. Pertanto, il tempo di esecuzione nel caso peggiore è $O(n \cdot m)$.

\clearpage
\subsection*{Esempio}
Supponiamo di avere un testo 
$$ T = \text{"abacaabaccabacabaabb"} $$
e un pattern 
$$ P = \text{"abacab"} $$

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/PatternMatching/ex_BruteForce.png}
    \caption{Esempio di Pattern Matching con algoritmo Brute Force. L'algoritmo esegue 27 confronti tra caratteri, numerati in figura.}
\end{figure}


\clearpage
\section{L'algoritmo di Boyer-Moore}
Come vedremo tra poco, non è sempre necessario confrontare ogni carattere del pattern con il testo. L'algoritmo di \emph{Boyer-Moore} sfrutta questa osservazione per saltare alcune posizioni nel testo, riducendo così il numero di confronti necessari. 

L'idea principale dell'algoritmo di \emph{Boyer-Moore} è di migliorare l'efficienza dell'algoritmo \emph{Brute Force} utilizzando due tecniche (euristiche) principali:

\begin{itemize}
    \item \textbf{Looking-Glass Heuristic}: Quando si confrontano i caratteri del pattern con il testo, si inizia dal carattere più a destra del pattern e si procede verso sinistra.
    \item \textbf{Character-Jump Heuristic}: Durante la verifica di un possibile piazzamento di $P$ in $T$, un mismatch tra $T[i] = c$ e $P[k]$ viene gestito come segue:
    
    Supponiamo che $T[i] \neq P[k]$ e $T[i] = c$.
        \begin{itemize}
            \item Se $c$ non appare in $P$, $P$ può essere "spostato" completamente oltre $T[i]$ ($P[0]$ viene allineato con $T[i+1]$).
            \item Altrimenti, $T[i]$ viene allineato con l'ultima occorrenza di $c$ in $P$.
        \end{itemize}
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/PatternMatching/ex_BoyerMoore.png}
    \caption{Una semplice dimostrazione dell'algoritmo di Boyer-Moore. Nel primo confronto si ha $T[4] \neq P[4]$ con $T[4] = \text{'e'}$ che non è presente in $P$, per cui spostiamo $P$ oltre $T[4]$. Nel secondo confronto si ha $T[9] \neq P[4]$ con $T[9] = \text{'s'}$ che è presente in $P$, in particolare l'ultima occorrenza di 's' è in $P[2]$, per cui allineiamo $P[2]$ con $T[9]$.}
    \label{exBoyerMoore}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/PatternMatching/ex2_BoyerMoore.png}
    \caption{Esempio completo che mostra il numero di confronti.}
    \label{ex2BoyerMoore}
\end{figure}

\clearpage
\noindent
Per formalizzare l'algoritmo di \emph{Boyer-Moore} possiamo generalizzare il funzionamento come di seguito:

\begin{itemize}
    \item Quando viene trovata una corrispondenza (a partire dall'ultimo carattere del pattern), l'algoritmo continua cercando di estendere la corrispondenza con il penultimo carattere del pattern nel suo allineamento corrente. Questo processo continua fino a quando tutti i caratteri del pattern sono stati confrontati con esito positivo o fino a quando si verifica un mismatch.
    \item Quando si verifica un mismsatch, e il carattere del testo che ha causato il mismatch non è presente nel pattern, il pattern viene spostato completamente oltre quel carattere del testo. Se il carattere del testo è presente da qualche altra parte nel pattern, dobbiamo considerare due diversi casi a seconda che la sua ultima occorrenza (nel pattern) sia $(a)$ precedente o $(b)$ successiva al carattere del pattern che era allineato con il carattere del testo che ha causato il mismatch.
\end{itemize}

Questi due casi sono rappresentati in figura \ref{fig:BM_mismatch_cases} e approfonfiti di seguito:
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/PatternMatching/BM_mismatch_cases.png}
    \caption{Indichiamo con $i$ l'indice del carattere non corrispondente nel testo, con $k$ l'indice corrispondente nel pattern e con $j$ l'indice dell' ultima occorrenza di $T[i]$ all'interno del pattern. Distinguiamo due casi: \textbf{(a)} $j < k$, nel qual caso spostiamo il pattern di $k-j$ unità, e quindi l'indice $i$ avanza di $m-(j +1)$ unità; \textbf{(b)} $j > k$, nel qual caso spostiamo il pattern di un'unità, e l'indice $i$ avanza di $m-k$ unità.
    N.B. $m$ è la lunghezza del pattern.}
    \label{fig:BM_mismatch_cases}
\end{figure}

\clearpage
\noindent
Nel caso di \ref{fig:BM_mismatch_cases}(b), il pattern viene spostato di una sola unità a destra. Sarebbe sicuramente più produttivo spostare il pattern a destra fino a quando l'ultima occorrenza di $T[i]$ nel pattern non è allineata con $T[i]$, ma questo richiederebbe un ulteriore calcolo per la ricerca della nuova occorrenza. 

L'efficienza dell'algoritmo di \emph{Boyer-Moore} risiede nell'utilizzo di una funzione di pre-elaborazione, $L(c)$, chiamata \textbf{last occurrence function} che consente di determinare rapidamente l'ultima occorrenza di un carattere nel pattern. Questa funzione viene calcolata una sola volta prima dell'inizio del processo di ricerca e viene utilizzata ogni volta che si verifica un mismatch.
$$
\forall c \in \Sigma, \quad L(c) = 
\begin{cases} 
   \max \{ i \mid P[i] = c \} & \text{se } c \in P \\
   -1 & \text{se } c \notin P 
\end{cases}
$$

\noindent
Per esempio, se abbiamo 
$\Sigma = \{ \text{'a'}, \text{'b'}, \text{'c'}, \text{'d'} \}$ 
e 
$P = \text{"abacab"}$
:
\begin{table}[h]
    \centering
    % Aumenta lo spazio orizzontale (default è 6pt)
    \setlength{\tabcolsep}{18pt} 
    % Aumenta lo spazio verticale (default è 1)
    \renewcommand{\arraystretch}{1.5}

    \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{c} & \textbf{'a'} & \textbf{'b'} & \textbf{'c'} & \textbf{'d'} \\
        \hline
        \textbf{$L(c)$} & 4 & 5 & 3 & -1 \\
        \hline
    \end{tabular}
\end{table}

\noindent
Possiamo modellare $L$ come una \emph{Map} che ha per chiavi i caratteri dell'alfabeto e per valori gli indici delle loro ultime occorrenze nel pattern (ad esempio, in Python possiamo usare un dizionario).
La Map $L$ può essere costruita in tempo $O(m + |\Sigma|)$ dove $m$ è la lunghezza del pattern e $|\Sigma|$ è la dimensione dell'alfabeto.

\vspace{1\baselineskip}
\begin{lstlisting}
def last_occurrence(p, sigma):
    """Return the last-occurrence map L for pattern p over alphabet sigma."""
    L = {c:-1 for c in sigma}  # initialize all characters to -1
    for i in range(len(p)):
        L[p[i]] = i  # update with last occurrence of character p[i]
    return L
\end{lstlisting}

\clearpage
\begin{lstlisting}
def find_boyer_moore(T, P):
    """Restituisce l'indice più basso in T dove inizia P (altrimenti -1)."""
    n, m = len(T), len(P)   # Notazioni comode per le lunghezze
    if m == 0:
        return 0            # Ricerca banale per pattern vuoto
    
    # last occurrence function
    last = {}               # Costruisce la mappa delle ultime occorrenze
    for k in range(m):
        last[P[k]] = k      # Le occorrenze successive sovrascrivono le precedenti

    # Allinea la fine del pattern all'indice m-1 del testo
    i = m-1                 # Indice corrente nel Testo (T)
    k = m-1                 # Indice corrente nel Pattern (P)
    
    while i < n:
        if T[i] == P[k]:    # Carattere corrispondente trovato
            if k == 0:      
                return i    # Trovato! Il pattern inizia all'indice i del testo
            else:
                i -= 1      # Estendi la corrispondenza a sinistra
                k -= 1      # Sposta entrambi gli indici
        else:
            # Mismatch: gestione del salto
            j = last.get(T[i], -1)  # Indice dell'ultima occorrenza di T[i] nel pattern
            
            # Calcolo del salto (shift) combinando i casi:
            # min(k, j+1) gestisce la logica dei Casi A e B:
            # - Se j < k (Caso A): allinea l'occorrenza j con il testo.
            # - Se j > k (Caso B): evita shift negativo, sposta il pattern di 1 posizione.
            i += m - min(k, j+1)    
            
            k = m-1         # Riavvia il confronto dalla fine del pattern
            
    return -1    
\end{lstlisting}
\vspace{1\baselineskip}


\clearpage
\subsection*{Performance}
La versione semplificata dell'algoritmo di Boyer-Moore presentata qui ha un tempo di esecuzione di $O(n \cdot m + |\Sigma|)$, in quanto la last-occurrence function richiede $O(m + |\Sigma|)$ tempo per essere costruita e la ricerca del pattern richiede $O(n \cdot m)$. 

Il caso peggiore si verifica quando si ha una coppia del tipo: 
$$T = \text{"aaa...aaa"} \quad P = \text{"baa...aaa"}$$
In questo caso, l'algoritmo di Boyer-Moore si comporta come l'algoritmo Brute Force, eseguendo $O(n \cdot m)$ confronti tra caratteri.

Tuttavia, l'algoritmo originale di Boyer-Moore utilizza euristiche più avanzate ed efficienti che consentono di ottenere un tempo di esecuzione medio di $O(n + m + |\Sigma|)$.


\subsection*{Esempio}
Supponiamo di avere un testo 
$$ T = \text{"abacaabadcabacabaabb"} $$
e un pattern 
$$ P = \text{"abacab"} $$

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/PatternMatching/BM_Lfunction.png}
    \caption{Esempio di Pattern Matching con algoritmo di Boyer-Moore, con anche la tabella delle last occurrence. L'algoritmo esegue 13 confronti tra caratteri, numerati in figura.}
    \label{fig:BM_Lfunction}
\end{figure}





\section{L'algoritmo di Knuth-Morris-Pratt}
Analizzando le prestazioni nel caso peggiore degli algoritmi di pattern-matching \emph{brute-force} e \emph{Boyer-Moore} su istanze specifiche del problema possiamo notare una notevole inefficienza. Per un certo allineamento del pattern, se troviamo diversi caratteri corrispondenti ma poi rileviamo un mismatch, ignoriamo tutte le informazioni ottenute dai confronti andati a buon fine.

L'algoritmo di \emph{Knuth-Morris-Pratt} (o “KMP”) evita questo spreco di informazioni e, facendo così, raggiunge un tempo di esecuzione di $O(n+m)$: $O(m)$ per il pre-processing del pattern, e $O(n)$ per i confronti tra testo e pattern. Nel caso peggiore qualsiasi algoritmo di pattern-matching dovrà esaminare tutti i caratteri del testo e tutti i caratteri del pattern almeno una volta. L'idea principale dell'algoritmo KMP è quella di pre-calcolare le sovrapposizioni del pattern su se stesso. In questo modo, quando si verifica un mismatch in una certa posizione, sappiamo immediatamente qual è lo spostamento massimo che possiamo applicare al pattern prima di continuare la ricerca.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/PatternMatching/KMP_first_example.png}
    \caption{KMP non ripete i confronti sui primi tre caratteri del pattern dal momento che si ripetono e sono già stati confrontati con successo.}
    \label{fig:KMP_first_example}
\end{figure}

\subsection{Failure-Function}
L'algoritmo KMP si basa sul calcolo della \textbf{failure function} $f$, che indica il corretto spostamento di $P$ in caso di confronto fallito. In particolare, la funzione di fallimento $f(k)$ è definita come la \textbf{lunghezza del più lungo prefisso di $P$ che è anche un suffisso di $P[1:k+1]$}\footnote{P[1:k+1] comprende i caratteri da indice $1$ a indice $k$, $k+1$ non è incluso. Nota che non abbiamo incluso $P[0]$ qui, dal momento che una stringa è suffisso di sè stesso; così facendo imponiamo che sia un \textbf{suffisso proprio}, cioè diverso dalla stringa stessa}. Intuitivamente, se troviamo un mismatch sul carattere $P[k+1]$, la funzione $f(k)$ ci dice quanti dei caratteri immediatamente precedenti possono essere riutilizzati per riavviare il pattern. 

Operativamente, la funzione viene utilizzata nel seguente modo: se durante il confronto si verifica un \textit{mismatch} all'indice $j$ del pattern (cioè $P[j] \neq T[i]$), significa che i caratteri precedenti $P[0 \dots j-1]$ corrispondevano al testo. Invece di ripartire da zero, l'algoritmo consulta il valore $f(j-1)$, il quale ci indica quanti caratteri del prefisso possiamo "salvare". Il confronto riprenderà quindi confrontando il carattere del testo $T[i]$ con il nuovo indice del pattern $j' = f(j-1)$.

\clearpage
\noindent
Per esempio, consideriamo il pattern $P = \text{"amalgamation"}$. La failure function $f$ per questo pattern è mostrata nella tabella seguente:

\begin{table}[h]
    \centering
    % Definisce: 1 colonna a sinistra, linea verticale, 12 colonne centrate
    \begin{tabular}{l|*{12}{c}}
        \textbf{$k$} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 \\
        \hline
        \textbf{$P[k]$} & a & m & a & l & g & a & m & a & t & i & o & n \\
        \hline
        \textbf{$f(k)$} & 0 & 0 & 1 & 0 & 0 & 1 & 2 & 3 & 0 & 0 & 0 & 0 \\
    \end{tabular}
\end{table}


\subsection*{Implementazione}
L'implementazione dell'algoritmo KMP si basa su una funzione di utilità, \texttt{compute\_kmp\_fail}, per calcolare efficientemente la funzione di fallimento. La parte principale dell'algoritmo KMP è il suo ciclo \texttt{while}, che per ogni iterazione esegue un confronto tra il carattere $T[j]$ e il carattere $P[k]$. Se l'esito di questo confronto è una corrispondenza, l'algoritmo procede ai caratteri successivi sia in $T$ che in $P$ (o segnala una corrispondenza completa se si raggiunge la fine del pattern). Se il confronto fallisce, l'algoritmo consulta la funzione di fallimento per un nuovo carattere candidato in $P$, o ricomincia con il prossimo indice in $T$ se si fallisce sul primo carattere del pattern (dato che nulla può essere riutilizzato).

\vspace{1\baselineskip}
\begin{lstlisting}
def find_kmp(T, P):
    """Return the lowest index of T at which substring P begins (or else -1)."""
    n, m = len(T), len(P)   # introduce convenient notations
    if m == 0:
        return 0            # trivial search of empty pattern
    
    fail = compute_kmp_fail(P)  # rely on utility to precompute fail function
    j = 0                   # index into text
    k = 0                   # index into pattern
    while j < n:
        if T[j] == P[k]:    # P[0:k] matched thus far
            if k == m-1:    # match is complete  
                return j - m + 1   # pattern begins at index (j-m+1) of text 
            j += 1          # try to extend match
            k += 1
        elif k > 0:
            k = fail[k-1]   # reuse suffix of P[0:k-1]
        else:
            j += 1          # no match at P[0], try next character in T
    return -1 
\end{lstlisting}


\clearpage
\begin{lstlisting}
def compute_kmp_fail(P):
    """Utility that computes and returns KMP fail list."""
    m = len(P)
    fail = [0] * m      # by default, presume overlap of 0 everywhere
    j = 1
    k = 0
    while j < m:        # compute f(j) during this pass, if nonzero
        if P[j] == P[k]:  # k+1 characters match thus far
            fail[j] = k + 1
            j += 1
            k += 1
        elif k > 0:     # k follows a matching prefix
            k = fail[k-1]
        else:           # no match found starting at j
            j += 1
    return fail
\end{lstlisting}
\vspace{1\baselineskip}

\subsection*{Performance}
Tralasciando momentaneamente il calcolo della failure function, il tempo di esecuzione dell'algoritmo KMP è chiaramente proporzionale al numero di iterazioni del ciclo while. Ai fini dell'analisi, definiamo $s = j-k$ come la quantità totale di spostamento del pattern $P$ rispetto al testo $T$. Notiamo che durante l'esecuzione dell'algoritmo, abbiamo sempre $s \leq n$. Ad ogni iterazione del ciclo si verifica uno dei seguenti tre casi:
\begin{itemize}
    \item Se $T[j] = P[k]$, allora sia $j$ che $k$ aumentano di 1, e quindi $s$ non cambia.
    \item Se $T[j] \neq P[k]$ e $k > 0$, allora $j$ non cambia e $s$ aumenta di almeno 1, poiché in questo caso $s$ cambia da $j-k$ a $j-f(k-1)$, che è un'aggiunta pari a $k-f(k-1)$, che è positiva perché $f(k-1) < k$.
    \item Se $T[j] \neq P[k]$ e $k = 0$, allora $j$ aumenta di 1 e $s$ aumenta di 1, poiché $k$ non cambia.
\end{itemize}

\noindent
Quindi, in ogni iterazione del ciclo, o $j$ o $s$ vengono incrementati di almeno 1 (eventualmente entrambi); pertanto, il numero totale di iterazioni del ciclo while nell'algoritmo KMP è al massimo $2n$. Per rendere tutto ciò possibile si presuppone che la failure function di $P$ sia già stata precedentemente calcolata.

L'algoritmo per il calcolo della failure function ha una complessità di $O(m)$. La sua analisi è analoga a quella del principale algoritmo KMP, ma esegue dei confronti tra il pattern di lunghezza $m$ con se stesso.

Dunque, combinando entrambi i risultati, otteniamo che l'algoritmo KMP ha una complessità temporale complessiva di $O(n + m)$.
La correttezza dell'algoritmo KMP deriva direttamente dalla definizione della failure function. Qualsiasi confronto che viene saltato è in realtà superfluo, poiché la funzione di fallimento garantisce che tutti i confronti ignorati siano ridondanti.

\clearpage
\subsection*{Esempio}
Supponiamo di avere un testo 
$$ T = \text{"abacaabaccabacabaabb"} $$
e un pattern
$$ P = \text{"abacab"} $$

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/PatternMatching/KMP_full_example.png}
    \caption{Esempio di Pattern Matching con algoritmo KMP, con anche la tabella della failure function. L'algoritmo esegue 19 confronti tra caratteri, numerati in figura (sono necessari ulteriori confronti per il calcolo della failure function non presenti in figura).}
    \label{KMP_full_example}
\end{figure}

