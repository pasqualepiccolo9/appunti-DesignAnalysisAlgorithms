\chapter{Minimum Spanning Trees (MST)}
\label{cap:MST}

È utile cominciare richiamando alcune definizioni già viste nel capitolo sui grafi (capitolo \ref{cap:Graphs}).
\begin{itemize}[nosep]
    \item Una \textbf{forest} è un grafo non orientato e aciclico (ovvero privo di cicli). 
    \item Un \textbf{tree} è una forest connessa. In altre parole, un tree è un grafo non orientato, aciclico e connesso.
    \item Un \textbf{sottografo} $H$ di un grafo $G = (V,E)$ è un grafo $H = (V', E')$ tale che $V' \subseteq V$ e $E' \subseteq E$. In altre parole, un sottografo è ottenuto rimuovendo vertici e/o archi da $G$. Uno \textbf{spanning subgraph} (la traduzione corretta è "sottografo ricoprente") di un grafo $G = (V,E)$ è un sottografo $H = (V', E')$ tale che $V' = V$. In altre parole, uno spanning subgraph contiene tutti i vertici di $G$, ma potrebbe non contenere tutti gli archi di $G$. 
    \item Uno \textbf{spanning tree} di un grafo è uno spanning subgraph che è anche un tree (tree = grafo non orientato, aciclico e connesso). Si noti come uno spanning tree di un grafo con $n$ vertici \textbf{contenga esattamente} $n-1$ archi.
\end{itemize}

\vspace{1\baselineskip}
\noindent
Dato un grafo non orientato e pesato $G$, siamo interessati a trovare un tree $T$ che contenga tutti i vertici di $G$ e minimizzi la somma dei pesi degli archi in $T$. 

$$ w(T) = \sum_{(u,v) \in T} w(u,v) $$

\noindent
Un tale tree $T$ è chiamato \textbf{minimum spanning tree} (MST) di $G$.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/MST/mst_ex.png}
    \caption{Esempio di grafo non orientato e pesato (a sinistra) e del suo Minimum Spanning Tree (a destra).}
    \label{fig:mst_ex}
\end{figure}

\clearpage
\noindent
Si tratta di un problema di ottimizzazione combinatoria molto importante, con numerose applicazioni pratiche, ad esempio nella progettazione di reti (telefoniche, elettriche, idriche, ecc.). 

Supponiamo, ad esempio, di dover connettere tutti i computer di un nuovo edificio per uffici utilizzando la minor quantità di cavo possibile. Possiamo modellare questo problema utilizzando un grafo non orientato e pesato $G$ i cui vertici rappresentano i computer, e i cui archi rappresentano tutte le possibili coppie $(u,v)$ di computer, dove il peso $w(u,v)$ dell'arco $(u,v)$ è pari alla quantità di cavo necessaria per connettere il computer $u$ al computer $v$. Piuttosto che calcolare un albero dei cammini minimi a partire da un particolare vertice $v$, siamo invece interessati a trovare un albero $T$ che contenga tutti i vertici di $G$ e che abbia il peso totale minimo tra tutti gli alberi di questo tipo. 

\paragraph{MST vs Shortest Paths:} è importante non confondere il problema del Minimum Spanning Tree con il problema degli Shortest Paths. Gli spanning tree calcolati dagli algoritmi di Shortest Paths (come Dijkstra o Bellman-Ford) dipendono dal vertice sorgente scelto, mentre il Minimum Spanning Tree è unico (a meno di casi particolari di pesi uguali) e non dipende da alcun vertice sorgente. Inoltre, gli Shortest Paths mirano a minimizzare la distanza da un singolo vertice sorgente a tutti gli altri vertici, mentre il Minimum Spanning Tree mira a minimizzare la somma totale dei pesi degli archi che connettono tutti i vertici del grafo.

\section*{Proprietà di un MST - Cycle Property}
Sia $T$ un Minimum Spanning Tree di un grafo non orientato e pesato $G$, e sia $e$ un arco di $G$ non appartenente a $T$. Chiamiamo $C$ il ciclo ottenuto aggiungendo l'arco $e$ a $T$. Allora, l'arco $e$ è il più pesante tra tutti gli archi del ciclo $C$, ovvero $w(f) \leq w(e)$ per ogni arco $f \in C$.

\paragraph{Dimostrazione:} 
Supponiamo, per assurdo, che esista un arco $f \in C$ tale che $w(f) > w(e)$. Rimuovendo l'arco $f$ da $T$ e aggiungendo l'arco $e$, otteniamo un nuovo albero $T' = T - \{f\} + \{e\}$. Poiché $w(e) < w(f)$, il peso totale di $T'$ è minore di quello di $T$, ovvero $w(T') < w(T)$. Questo contraddice l'ipotesi che $T$ sia un Minimum Spanning Tree. Pertanto, l'arco $e$ deve essere il più pesante tra tutti gli archi del ciclo $C$.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/MST/cycle_property.png}
    \caption{Dimostrazione della Cycle Property.}
    \label{fig:cycle_property}
\end{figure}


\clearpage
\section*{Proprietà di un MST - Partitioning Property}
Sia $G$ un grafo connesso, non orientato e pesato. Sia $V_1$ e $V_2$ una partizione dei vertici di $G$ in due insiemi disgiunti e non vuoti. Inoltre, sia $e$ un arco in $G$ con peso minimo tra quelli che hanno un estremo in $V_1$ e l'altro in $V_2$ (cioè tra quelli che fanno da "ponte" tra le due partizioni). Allora, esiste un\footnote
{
    Si dice \emph{un} Minimum Spanning Tree poiché, in presenza di archi con lo stesso peso, il MST non è necessariamente unico: l'arco $e$ di peso minimo che attraversa la partizione appartiene ad almeno un MST, ma non è detto che appartenga a tutti. Se tutti gli archi hanno pesi distinti, allora il MST è unico e l'arco $e$ appartiene necessariamente a questo unico MST.
}
Minimum Spanning Tree $T$ che ha $e$ come uno dei suoi archi.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/MST/partitioning_property.png}
    \caption{Dimostrazione della Partitioning Property.}
    \label{fig:partitioning_property}
\end{figure}

\paragraph{Dimostrazione:}
Sia $T$ un Minimum Spanning Tree di $G$. Se $e \in T$, la tesi è verificata.
Supponiamo quindi che $e \notin T$. Poiché $T$ è uno spanning tree, esiste un unico cammino in $T$ che collega gli estremi dell'arco $e$. L'aggiunta di $e$ a $T$ genera dunque un ciclo $C$.

Poiché l'arco $e$ ha un estremo in $V_1$ e l'altro in $V_2$, nel ciclo $C$ deve esistere almeno un arco $f \neq e$ appartenente a $T$ che ha un estremo in $V_1$ e l'altro in $V_2$. Per la definizione di $e$, si ha
\[
w(e) \leq w(f).
\]

\noindent
Consideriamo ora il grafo $T'$ ottenuto rimpiazzando l'arco $f$ con l'arco $e$ in $T$:
\[
T' = (T \cup \{e\}) - \{f\}.
\]

\noindent
Il grafo $T'$ è connesso, aciclico e copre tutti i vertici di $G$, quindi è uno spanning tree. 
Inoltre:
\[
w(T') = w(T) + w(e) - w(f) \leq w(T).
\]

\noindent
Poiché $T$ è un Minimum Spanning Tree, segue che $w(T') = w(T)$ e quindi anche $T'$ è un Minimum Spanning Tree. Per costruzione, $T'$ contiene l'arco $e$.

Pertanto, esiste un Minimum Spanning Tree di $G$ che contiene l'arco $e$.


\clearpage
\section*{L'approccio greedy per il calcolo di un MST}
L'idea più immediata per calcolare un Minimum Spanning Tree sarebbe quella di calcolare tutti i possibili spanning tree di un grafo e scegliere quello con peso minimo. Tuttavia, questo approccio è computazionalmente inefficiente, poiché il numero di spanning tree cresce esponenzialmente con il numero di vertici del grafo, per cui per un grafo con $n$ vertici il numero di spanning tree possibili è dell'ordine di $O(n^{n-2})$.

Ricordando che uno spanning tree con $n$ vertici contiene esattamente $n-1$ archi, possiamo considerare un approccio più efficiente basato sulla selezione iterativa degli archi da includere nello spanning tree. In particolare, possiamo adottare un approccio greedy, che consiste nel costruire la soluzione passo dopo passo, a partire dalla soluzione vuota, selezionando ad ogni passo l'arco (o il vertice) da aggiungere alla soluzione in base a una scelta che minimizza una certa funzione di costo. 

L'idea generale è quella di scegliere, ad ogni step, l'arco di peso minimo che preservi la proprietà di \textbf{aciclicità}.

\vspace{1\baselineskip}
\noindent
Di seguito presentiamo tre possibili algoritmi greedy per il calcolo di un Minimum Spanning Tree: l'algoritmo di \textbf{Prim-Jarník}, l'algoritmo di \textbf{Kruskal} e il \textbf{Reverse-Delete Algorithm}.


\section{L'algoritmo di Prim-Jarník}
L'algoritmo di Prim-Jarník (spesso abbreviato in Prim) è un algoritmo greedy per il calcolo di un Minimum Spanning Tree di un grafo connesso, non orientato e pesato.

L'algoritmo consiste nel costruire un minimum spanning tree partendo da un singolo cluster partendo da un vertice "radice" $s$. L'idea principale è simile a quella dell'algoritmo di Dijkstra. Iniziamo con un vertice $s$ che definisce la "nuvola" (simile a Dijkstra) iniziale di vertici $C$. Poi, ad ogni iterazione, scegliamo un arco di peso minimo $e = (u,v)$ che collega un vertice $u$ nella nuvola $C$ a un vertice $v$ al di fuori di $C$. Il vertice $v$ viene quindi aggiunto alla nuvola $C$ e il processo viene ripetuto fino a formare uno spanning tree. Ancora una volta, entra in gioco il fatto cruciale riguardante i minimum spanning tree: scegliendo sempre l'arco di peso minimo che collega un vertice all'interno di $C$ a uno all'esterno di $C$, siamo sicuri di aggiungere sempre un arco valido al MST.

Per implementare in modo efficiente questo approccio, possiamo prendere ancora una volta spunto dall'algoritmo di Dijkstra. Manteniamo un'etichetta $D[v]$ per ogni vertice $v$ al di fuori della nuvola $C$, in modo che $D[v]$ memorizzi il peso dell'arco minimo osservato per collegare $v$ alla nuvola $C$. (Nell'algoritmo di Dijkstra, questa etichetta misurava la lunghezza totale del percorso dal vertice di partenza $s$ a $v$, incluso un arco $(u,v)$.) Queste etichette servono come chiavi in una coda di priorità utilizzata per decidere quale vertice è il prossimo ad unirsi alla nuvola.

\clearpage
\subsection*{Pseudocodice}
L'algoritmo Prim è descritto nel seguente pseudocodice:
\vspace{1\baselineskip}
\hrule
\begin{verbatim}
Input: An undirected, weighted, connected graph G with n vertices 
        and m edges
Output: A minimum spanning tree T for G
1.  Algorithm PrimJarnik(G):
2.      Pick any vertex s of G
3.      D[s] = 0
4.      for each vertex v != s do
5.          D[v] = +infinity
6.      Initialize T = empty set
7.      Initialize a priority queue Q with an entry (D[v], (v,None))  
            for each vertex v, where D[v] is the key in the priority  
            queue, and (v,None) is the associated value.
8.      while Q is not empty do
9.          (u,e) = value returned by Q.remove min()
10.         Connect vertex u to T using edge e.
11.         for each edge e' = (u,v) such that v is in Q do
12.             # check if edge (u,v) better connects v to T
13.             if w(u,v) < D[v] then
14.                 D[v] = w(u,v)
15.                 Change the key of vertex v in Q to D[v].
16.                 Change the value of vertex v in Q to (v,e').
17.     return the tree T     
\end{verbatim}
\hrule 
\vspace{1\baselineskip}

\subsection*{Analisi della complessità}
I tempi di esecuzione dell'implementazione dell'algoritmo Prim sono gli stessi di quelli dell'algoritmo di Dijkstra, poichè entrambi dipendono dalle operazioni eseguite su una Adaptable Priority Queue $Q$. Inizialmente vengono eseguiti $n$ inserimenti in $Q$, e successivamente vengono eseguite $n$ operazioni di \texttt{remove\_min()}, e possono essere aggiornate un totale di $m$ priorità come parte dell'algoritmo. Questi passaggi sono i principali contributi al tempo di esecuzione complessivo. Con una coda di priorità basata su heap, ogni operazione viene eseguita in tempo $O(\log n)$, e il tempo complessivo per l'algoritmo è $O((n+m) \log n)$, che è $O(m \log n)$ per un grafo connesso. In alternativa, possiamo ottenere un tempo di esecuzione di $O(n^2)$ utilizzando una lista non ordinata (unsorted list) come coda di priorità.


\clearpage
\subsection*{Implementazione Python}
\vspace{2\baselineskip}
\begin{lstlisting}[
    language=Python,
    caption={Implementazione Python dell'algoritmo Prim. L'MST viene restituito come una lista non ordinata di archi.},
    captionpos=b,
    label={lst:Prim},
]
def MST_PrimJarnik(g):
    """Compute a minimum spanning tree of weighted graph g.

    Return a list of edges that comprise the MST (in arbitrary order).
    """
    d = { }                  # d[v] is bound on distance to tree
    tree = [ ]               # list of edges in spanning tree
    pq = AdaptableHeapPriorityQueue( )  # d[v] maps to value (v, e=(u,v))
    pqlocator = { }          # map from vertex to its pq locator

    # for each vertex v, add an entry to the priority queue, with
    # the source having distance 0 and all others having infinite distance
    for v in g.vertices( ):
        if len(d) == 0:      # this is the first node
            d[v] = 0         # make it the root
        else:
            d[v] = float('inf')  # positive infinity
        pqlocator[v] = pq.add(d[v], (v,None))

    while not pq.is_empty( ):
        key,value = pq.remove_min()
        u,edge = value         # unpack tuple from pq
        del pqlocator[u]       # u is no longer in pq
        if edge is not None:
            tree.append(edge)  # add edge to tree
        for link in g.incident_edges(u):
            v = link.opposite(u)
            if v in pqlocator:  # thus v not yet in tree
                # see if edge (u,v) better connects v to the growing tree
                wgt = link.element( )
                if wgt < d[v]:   # better edge to v?
                    d[v] = wgt   # update the distance
                    pq.update(pqlocator[v], d[v], (v, link))  # update the pq entry
    return tree
\end{lstlisting}

\clearpage
\subsection*{Esempio di esecuzione}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/MST/prim1.png}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/MST/prim2.png}
    \caption{Esempio di esecuzione dell'algoritmo Prim su un grafo connesso, non orientato e pesato.}
    \label{fig:prim_ex}
\end{figure}


\clearpage
\section{L'algoritmo di Kruskal}
L'algoritmo di Kruskal è un altro algoritmo greedy per la risoluzione del problema del Minimum Spanning Tree. A differenza dell'algoritmo di Prim-Jarník, che costruisce l'albero facendo crescere un unico cluster (o "nuvola") di vertici connessi, l'algoritmo di Kruskal mantiene una \textbf{foresta di alberi disgiunti} (cluster) che vengono progressivamente fusi insieme.

Inizialmente, ogni vertice del grafo è considerato un albero separato (un cluster elementare). L'algoritmo esamina gli archi del grafo in ordine crescente di peso. Per ogni arco considerato, se questo connette due vertici che appartengono a cluster diversi, l'arco viene aggiunto alla soluzione e i due cluster vengono fusi in un unico insieme. Se invece l'arco connette due vertici che appartengono già allo stesso cluster, l'arco viene scartato, poiché la sua aggiunta creerebbe un ciclo (violando la proprietà fondamentale di aciclicità degli alberi).

L'algoritmo termina quando tutti i vertici appartengono allo stesso cluster (ovvero quando la foresta si riduce a un singolo albero che copre tutti i nodi) o quando non ci sono più archi da esaminare (se il grafo iniziale è non connesso, il risultato sarà una foresta di alberi minimi, ovvero un Minimum Spanning Forest, MSF).

\paragraph{Strutture Dati:}
Per implementare efficientemente questo approccio, sono necessarie due strutture dati principali:
\begin{enumerate}[nosep]
    \item Una \textbf{Priority Queue} per gestire gli archi ed estrarli in ordine di peso minimo.
    \item Una struttura dati per la gestione di \textbf{insiemi disgiunti} (Disjoint Sets), spesso chiamata \textbf{ADT Partition}.
\end{enumerate}

\noindent
L'ADT Partition deve supportare le seguenti operazioni:
\begin{itemize}[nosep]
    \item \texttt{makeSet(u)}: crea un nuovo insieme contenente solo il vertice $u$.
    \item \texttt{find(u)}: restituisce l'identificativo dell'insieme a cui appartiene $u$.
    \item \texttt{union(A, B)}: unisce i due insiemi $A$ e $B$ in un unico insieme, aggiornando i riferimenti.
\end{itemize}

\noindent
Una implementazione efficiente dell'ADT Partition (basata su liste con puntatori al set di appartenenza e euristica "weighted union" che sposta sempre l'insieme più piccolo in quello più grande) permette di eseguire le operazioni di unione in modo molto rapido.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.93\textwidth]{immagini/MST/partition.png}
    \caption{L'ADT Partition può essere rappresentato con un insieme di liste; ogni insieme è rappresentato da una lista distinta e ogni elemento nella lista ha un riferimento all'insieme a cui appartiene.}    
    \label{fig:partition}
\end{figure}


\clearpage
\subsection*{Pseudocodice}
L'algoritmo Kruskal è descritto nel seguente pseudocodice:
\vspace{1\baselineskip}
\hrule
\begin{verbatim}
Input: A simple connected weighted graph G with n vertices and m edges
Output: A minimum spanning tree T for G
1.  Algorithm Kruskal(G):
2.      for each vertex v in G do
3.          Define an elementary cluster C(v) = {v}.
4.      Initialize a priority queue Q to contain all edges in G, using 
            the weights as keys.
5.      T = empty set  {T will ultimately contain the edges of the MST}
6.      while T has fewer than n - 1 edges do
7.          (u,v) = value returned by Q.remove_min()
8.          Let C(u) be the cluster containing u
9.          Let C(v) be the cluster containing v
10.         if C(u) != C(v) then
11.             Add edge (u,v) to T.
12.             Merge C(u) and C(v) into one cluster.
13.     return tree T
\end{verbatim}
\hrule 
\vspace{1\baselineskip}

\subsection*{Analisi della complessità}
La complessità temporale dell'algoritmo di Kruskal dipende dall'implementazione della coda di priorità e della struttura Partition.
\begin{itemize}
    \item \textbf{Operazioni sulla Priority Queue:} La costruzione della coda richiede $O(m)$. L'estrazione degli archi richiede $O(\log m) = O(\log n)$ per operazione. Nel caso peggiore, estraiamo tutti gli archi, portando il costo globale delle operazioni sulla coda a $O(m \log m)$ o equivalentemente $O(m \log n)$.
    \item \textbf{Operazioni sui Cluster (Partition):} Vengono eseguite $2m$ operazioni di \texttt{find} che impiegano $O(m)$ e $n-1$ operazioni di \texttt{union} che impiegano $O(n \log n)$. Utilizzando un'implementazione efficiente basata su liste in cui si unisce sempre l'insieme più piccolo a quello più grande, il costo ammortizzato totale è $O(m + n \log n)$.
    \begin{itemize}
        \item La complessità temporale dell'algoritmo di Kruskal è quindi $O((n + m) \log n)$ se il grafo è rappresentato tramite liste di adiacenza/mappe, oppure $O(m \log n)$ se il grafo è connesso.
    \end{itemize}
\end{itemize}

\noindent
Complessivamente, il tempo di esecuzione è dominato dall'ordinamento degli archi (gestione della Priority Queue), risultando in una complessità di $O(m \log n)$ per un grafo connesso (dove $m \geq n-1$).

\clearpage
\subsection*{Implementazione Python}
Di seguito viene mostrata un'implementazione in Python che utilizza una classe \texttt{Partition} (non mostrata nel dettaglio, ma conforme all'ADT descritto) e una \texttt{HeapPriorityQueue}.

\vspace{2\baselineskip}
\begin{lstlisting}[
    language=Python,
    caption={Implementazione Python dell'algoritmo di Kruskal.},
    captionpos=b,
    label={lst:Kruskal},
]
def MST_Kruskal(g):
    """Compute a minimum spanning tree of a graph using Kruskal s algorithm.

    Return a list of edges that comprise the MST.
    
    The elements of the graph s edges are assumed to be weights.
    """
    
    tree = []                   # list of the edges in the MST
    pq = HeapPriorityQueue()    # keys are weights, elements are edges
    forest = Partition()        # forest of MST
    position = {}               # associates each vertex to its entry

    # Initialize partition: each vertex is a separate group
    for v in g.vertices():
        position[v] = forest.make_group(v)

    # Insert all edges in pq
    for e in g.edges():
        pq.add(e.element(), e)

    size = g.vertex_count()
    # Loop until tree has n-1 edges or pq is empty
    while len(tree) != size - 1 and not pq.is_empty():
        weight, edge = pq.remove_min()  # extract edge with min weight
        u, v = edge.endpoints()
        
        # find the sets that u and v belong to
        a = forest.find(position[u])
        b = forest.find(position[v])
        
        if a != b:                      # if the two sets are different
            tree.append(edge)           # add edge in tree
            forest.union(a, b)          # merge the two sets

    return tree
\end{lstlisting}


\clearpage
\subsection*{Esempio di esecuzione}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.77\textwidth]{immagini/MST/kruskal1.png}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.77\textwidth]{immagini/MST/kruskal2.png}
    \caption{Esempio di esecuzione dell'algoritmo di Kruskal. Gli archi solidi rossi rappresentano la soluzione parziale (foresta), i verdi rappresentano gli archi che si stanno considerando in quel momento, gli archi tratteggiati sono quelli esaminati ma scartati.}
    \label{fig:kruskal_ex}
\end{figure}


\noindent
Da notare, infine, che l'algoritmo di Kruskal e l'algoritmo di Prim-Jarník, pur risolvendo lo stesso problema, presentano alcune differenze chiave:
\begin{itemize}[nosep]
    \item \textbf{Approccio:} Prim costruisce un singolo albero espandendo un cluster di vertici connessi, mentre Kruskal costruisce una foresta di alberi disgiunti che vengono fusi insieme.
    \item \textbf{Strutture Dati:} Prim utilizza una coda di priorità per gestire i vertici, mentre Kruskal utilizza una coda di priorità per gli archi e una struttura per insiemi disgiunti (Partition) per gestire i cluster.
    \item \textbf{Applicabilità:} Prim richiede che il grafo sia connesso, mentre Kruskal può essere applicato anche a grafi non connessi, restituendo un Minimum Spanning Forest. D'altra parte , Prim è più semplice da implementare e occupa meno spazio in memoria.
\end{itemize}


\clearpage
\section{Reverse-Delete Algorithm}
Un altro algoritmo greedy per il calcolo di un Minimum Spanning Tree è il \textbf{Reverse-Delete Algorithm}. Questo algoritmo inizia con l'intero grafo e rimuove iterativamente gli archi, partendo da quelli di peso massimo, purché la rimozione non causi la disconnessione del grafo. L'idea è che, rimuovendo gli archi più pesanti che non sono necessari per mantenere la connettività, si ottiene un Minimum Spanning Tree.

Si tratta di un approccio "al contrario" rispetto agli algoritmi di Prim e Kruskal, che costruiscono il MST aggiungendo archi. Invece, il Reverse-Delete Algorithm parte da un grafo completo e rimuove archi fino a ottenere uno spanning tree. Anche in questo caso, come per l'algoritmo Prim, il grafo deve essere connesso affinché l'algoritmo funzioni correttamente.

\subsection*{Pseudocodice}
L'algoritmo Reverse-Delete è descritto nel seguente pseudocodice:

\vspace{1\baselineskip}
\hrule
\begin{verbatim}
Input: A simple connected weighted graph G with n vertices and m edges
Output: A minimum spanning tree T for G
1.  Algorithm ReverseDelete(G):
2.      T = set of all edges in G
3.      Initialize a priority queue Q to contain all edges in G, using 
            the weights as keys. {Q is a Max-Priority Queue}
4.      while Q is not empty do
5.          (u,v) = value returned by Q.remove_max()
6.          Remove edge (u,v) from T
7.          if there is no path between u and v in the graph (V, T) then
8.              Add edge (u,v) back to T  {(u,v) is a bridge}
9.      return tree T
\end{verbatim}
\hrule 
\vspace{1\baselineskip}

\subsection*{Analisi della complessità}
A differenza degli algoritmi di Prim e Kruskal, l'algoritmo Reverse-Delete è generalmente meno efficiente a causa del costo elevato per la verifica della connettività.

\begin{itemize}[nosep]
    \item \textbf{Costruzione della Priority Queue:} L'inizializzazione della coda con tutti gli $m$ archi richiede tempo $O(m)$.
    \item \textbf{Estrazione degli archi:} Il ciclo \texttt{while} itera $m$ volte. In ogni iterazione, l'operazione \texttt{remove\_max} impiega $O(\log m)$, portando il costo totale di questa fase a $O(m \log m)$.
    \item \textbf{Verifica della connettività:} Per ogni arco estratto, la verifica dell'esistenza di un cammino tramite una visita (BFS o DFS) impiega $O(n+m)$.
\end{itemize}

\noindent
Nonostante l'efficienza nella gestione della coda, il costo computazionale è dominato dalla verifica della connettività ripetuta per ogni arco. Pertanto, la complessità globale è $O(m \cdot (n+m))$, ovvero $O(m^2)$ nel caso di grafi densi.




