\chapter{Shortest Paths}
\label{cap:ShortestPaths}

Come abbiamo già visto, la ricerca in ampiezza (BFS) permette di trovare il cammino minimo in un grafo quando tutti gli archi sono considerati equivalenti. Tuttavia, in molte applicazioni reali questa condizione non è soddisfatta: per individuare il percorso più rapido tra due città o il tragitto più veloce per un pacchetto dati in rete, è necessario distinguere tra archi con caratteristiche diverse. Infatti, in questi contesti accade che alcune distanze tra le città saranno probabilmente molto più grandi di altre, e alcune connessioni in una rete di computer sono tipicamente molto più veloci di altre (ad esempio, alcune connessioni a bassa larghezza di banda contro connessioni ad alta velocità in fibra ottica). È quindi naturale considerare dei grafi pesati dove a ogni arco è associato un peso che ne rappresenta il costo o l'importanza.

\section{Weighted Graphs}
Un \textbf{weighted graph} (grafo pesato) è un grafo che ha un'etichetta numerica $w(e)$ associata a ciascun arco $e$, chiamata il peso (weight) dell'arco $e$. Per un arco $e = (u,v)$, indichiamo il suo peso con la notazione $w(e) = w(u,v)$. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/ShortestPath/weighted_graph.png}
    \caption{Esempio di grafo pesato i cui vertici rappresentano i principali aeroporti degli Stati Uniti e i cui pesi degli archi rappresentano le distanze in miglia.}
    \label{fig:weighted_graph}
\end{figure}


\clearpage
\noindent
Sia $G$ un grafo pesato. La lunghezza (o peso) di un percorso $P$ è la somma dei pesi degli archi di $P$. Cioè, se $P = ((v_0,v_1), (v_1,v_2), \ldots , (v_{k-1},v_k))$, allora la lunghezza di $P$, indicata con $w(P)$, è definita come:

$$
w(P) = \sum_{i=1}^{k} w(v_{i-1}, v_i)
$$

\noindent
La \textbf{distanza} da un vertice $u$ a un vertice $v$ in $G$, indicata con $d(u,v)$, è la lunghezza di un percorso di lunghezza minima (chiamato anche \textbf{shortest path}) da $u$ a $v$, se tale percorso esiste.
Se non esiste alcun percorso da $u$ a $v$, allora utilizziamo la convenzione $d(u,v) = \infty$.

\paragraph{Proprietà di uno shortest path} 
\begin{enumerate}
    \item Un frammento di uno shortest path è anch'esso uno shortest path. Cioè, se $P$ è uno shortest path da $a$ a $c$, e $b$ è un vertice su $P$, allora il frammento di $P$ da $a$ a $b$ è uno shortest path da $a$ a $b$, e il frammento di $P$ da $b$ a $c$ è uno shortest path da $b$ a $c$.
    \item Gli shortest path da un vertice sorgente $s$ a tutti gli altri vertici costituiscono uno \textbf{shortest path spanning tree} (o shortest path tree) radicato in $s$.
\end{enumerate}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/ShortestPath/shortest_path_tree.png}
    \caption{Esempio di uno shortest path tree radicato nel vertice PVD.}
    \label{fig:ex_shortest_path}
\end{figure}

\paragraph{Problemi legati agli shortest path} 
\begin{itemize}
    \item \textbf{(u, v)-shortest path}\\
    Dati due vertici $u$ e $v$, trovare lo shortest path tra $u$ e $v$.
    \item \textbf{Single source shortest path}\\
    Dato un vertice $s$, trovare lo shortest path tree radicato in $s$ (ovvero, trova lo shortest path da $s$ a tutti gli altri vertici).
    \item \textbf{All-pairs shortest paths}\\
    Trovare gli shortest path per tutte le coppie di vertici nel grafo.
\end{itemize}

\noindent
Si noti come dalla soluzione del problema single source shortest path sia possibile ottenere la soluzione del problema \emph{$(u, v)$-shortest path}, mentre dalla soluzione del problema \emph{all-pairs shortest paths} sia possibile ottenere la soluzione di entrambi gli altri problemi. Tuttavia, è anche possibile risolvere il problema \emph{all-pairs shortest paths} eseguendo l'algoritmo per il \emph{single source shortest path} per ogni vertice del grafo.

\clearpage
\section{Algoritmo di Dijkstra}
Si vuole sviluppare un \textbf{algoritmo Greedy} per la risoluzione del problema \emph{single source shortest path}. L'idea principale nell'applicare il \emph{metodo Greedy} al problema degli shortest path da una singola sorgente è quella di eseguire una "ricerca in ampiezza pesata" a partire dal vertice sorgente $s$. In particolare, possiamo utilizzare il \emph{metodo Greedy} per sviluppare un algoritmo che cresce iterativamente una "nuvola" di vertici a partire da $s$, con i vertici che entrano nella nuvola in ordine di distanza da $s$. Quindi, in ogni iterazione, il prossimo vertice scelto è il vertice al di fuori della nuvola che è più vicino a $s$. L'algoritmo termina quando non ci sono più vertici al di fuori della nuvola (o quando quelli al di fuori della nuvola non sono connessi a quelli all'interno della nuvola, e quindi hanno distanza infinita), a quel punto abbiamo uno shortest path da $s$ a ogni vertice di $G$ raggiungibile da $s$. Questo approccio è un esempio semplice, ma comunque potente, del \emph{metodo Greedy}. 

L'applicazione del \emph{metodo Greedy} al problema degli shortest path da una singola sorgente porta a un algoritmo noto come \textbf{algoritmo di Dijkstra}.


\subsection{Edge Relaxation - Greedy Choice}
Definiamo un'etichetta $D(v)$ per ogni vertice $v$ in $V$, che utilizziamo per approssimare la distanza da $s$ a $v$ in $G$. Il significato di queste etichette è che $D(v)$ memorizzerà sempre la lunghezza del miglior percorso che abbiamo trovato finora da $s$ a $v$. Difatti, possiamo pensare a $D(v)$ come un lmite superiore della distanza minima reale $d(s,v)$. Inoltre, definiamo un insieme $C$, che rappresenta l'insieme di vertici nella "nuvola", ovvero quei vertici per i quali abbiamo già calcolato la distanza minima da $s$.

Inizialmente, definiamo $D(s) = 0$ e $D(v) = \infty$ per ogni $v \neq s$, e l'insieme $C$ come l'insieme vuoto. Ad ogni iterazione dell'algoritmo, selezioniamo un vertice $u$ non in $C$ con l'etichetta $D(u)$ più piccola, e lo aggiungiamo a $C$ (Generalmente, utilizzeremo una coda di priorità per selezionare tra i vertici al di fuori della nuvola, per via dell'efficienza del metodo \texttt{remove\_min()}). 

Nella prima iterazione, ovviamente, aggiungeremo $s$ a $C$. Una volta che un nuovo vertice $u$ viene aggiunto a $C$, aggiorniamo l'etichetta $D(v)$ di ogni vertice $v$ adiacente a $u$ e che è al di fuori di $C$, per riflettere il fatto che potrebbe esserci un nuovo e migliore modo per raggiungere $v$ passando per $u$. Questa operazione di aggiornamento è nota come \emph{rilassamento degli archi} (\textbf{edge relaxation}), poiché prende una vecchia stima e verifica se può essere migliorata per avvicinarsi al suo valore reale. L'operazione specifica di rilassamento degli archi è la seguente:

\vspace{1\baselineskip}
$\quad \quad \quad \textbf{Edge Relaxation:}$
$$
\begin{aligned}
    &\textbf{if } D(u) + w(u,v) < D(v) \textbf{ then} \\
    &\quad D(v) = D(u) + w(u,v)
\end{aligned}
$$

\clearpage
\subsection{Pseudocodice ed Esempio}
L'algoritmo di Dijkstra è descritto nel seguente pseudocodice:
\vspace{1\baselineskip}
\hrule
\begin{verbatim}
Input: A weighted graph G with nonnegative edge weights, 
       and a source vertex s of G.
Output: The length of a shortest path from s to v for each 
        vertex v of G.
1.  Algorithm ShortestPath(G, s):   
2.      Initialize D[s] = 0 and D[v] = +infinity for each vertex v != s
3.      Let a priority queue Q contain all the vertices of G 
            using the D labels as keys.
4.      while Q is not empty do
5.          // pull a new vertex u into the cloud
6.          u = value returned by Q.remove_min()
7.          for each vertex v adjacent to u such that v is in Q do
8.              // perform the relaxation procedure on edge (u,v)
9.              if D[u] + w(u,v) < D[v] then
10.                 D[v] = D[u] + w(u,v)
11.                 Change to D[v] the key of vertex v in Q.
12.     return the label D[v] of each vertex v                  
\end{verbatim}
\hrule 
\vspace{1\baselineskip}

\clearpage

\vspace{2\baselineskip}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/ShortestPath/ex_dijkstra1.png}
    \label{fig:ex_dijkstra1}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/ShortestPath/ex_dijkstra2.png}
    \caption{Esempio di esecuzione dell'algoritmo di Dijkstra. L'esempio inizia con il vertice sorgente $A$ già nella nuvola (in grigio chiaro) e con le etichette dei vertici adiacenti aggiornate. Ad ogni passo, il vertice con l'etichetta minima viene aggiunto alla nuvola e le etichette dei suoi vicini vengono aggiornate di conseguenza. Al termine dell'algoritmo, le etichette rappresentano le distanze minime da $A$ a tutti gli altri vertici. Gli archi in rosso al di fuori della nuvola rappresentano il percorso minimo attuale verso quel vertice. Gli archi in rosso nella nuvola rappresentano lo shortest path tree radicato in $A$. Gli archi tratteggiati sono quelli che non fanno parte dello shortest path tree.}
    \label{fig:ex_dijkstra2}
\end{figure}

\clearpage

\subsection{Dimostrazione della correttezza dell'algoritmo}
Potrebbe non essere immediatamente chiaro il motivo per cui l'algoritmo di Dijkstra trovi correttamente lo shortest path dal vertice di partenza $s$ a ogni altro vertice $u$ nel grafo. Perché la distanza minima da $s$ a $u$ è uguale al valore dell'etichetta $D(u)$ al momento in cui il vertice $u$ viene rimosso dalla coda di priorità $Q$ e aggiunto alla nuvola $C$? La risposta a questa domanda dipende dal fatto che \textbf{non ci siano archi con peso negativo nel grafo}, poiché ciò consente al metodo Greedy di funzionare correttamente.

\paragraph{Teorema:} Nell'algoritmo di Dijkstra, ogni volta che un vertice $v$ viene inserito nella soluzione $C$, l'etichetta $D(v)$ è uguale a $d(s, v)$, la lunghezza del percorso minimo da $s$ a $v$.

\paragraph{Dimostrazione:} Per dimostrare la tesi, procediamo per assurdo. Ipotizziamo che l'algoritmo sbagli per qualche vertice. Sia $z$ il \textbf{primo vertice} che l'algoritmo inserisce in $C$ tale che la sua etichetta sia errata, ovvero:
$$ D(z) > d(s, z) $$
Poiché $z$ è il \textit{primo} errore, significa che per tutti i vertici inseriti in $C$ prima di $z$, l'algoritmo ha calcolato la distanza corretta.

Esiste sicuramente un percorso minimo reale da $s$ a $z$ (altrimenti la distanza sarebbe $\infty$). Chiamiamo questo percorso $P$.
Consideriamo il momento esatto in cui l'algoritmo sta per inserire $z$ in $C$. Analizziamo il percorso $P$ partendo da $s$:
\begin{itemize}
    \item Il vertice sorgente $s$ appartiene già a $C$.
    \item Il vertice $z$ non appartiene ancora a $C$.
    \item Di conseguenza, percorrendo il cammino $P$ da $s$ a $z$, deve necessariamente esistere un primo arco $(x, y)$ che attraversa il confine di $C$, cioè tale che $x \in C$ (il vertice "dentro") e $y \notin C$ (il primo vertice "fuori").
\end{itemize}
Dunque, sia $y$ il \textbf{primo vertice} di $P$ che non appartiene a $C$, e sia $x$ il predecessore di $y$ lungo il percorso $P$. 
\begin{itemize}
    \item Poiché $y$ è il primo vertice fuori da $C$, ne consegue che $x$ deve trovarsi dentro $C$ (al limite $x$ potrebbe coincidere con $s$).
\end{itemize}


\clearpage
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/ShortestPath/dimostrazione.png}
    \caption{Rappresentazione grafica della dimostrazione della correttezza dell'algoritmo di Dijkstra.}
    \label{fig:dijkstra_proof}
\end{figure}

\noindent
Poiché $z$ è stato definito come il \textit{primo} vertice "sbagliato" aggiunto a $C$, e $x$ è stato aggiunto a $C$ prima di $z$, allora l'etichetta di $x$ è sicuramente corretta:
$$ D(x) = d(s, x) $$

\noindent
Quando $x$ è stato inserito in $C$, l'algoritmo ha eseguito la procedura di \textit{Edge Relaxation} sui suoi vertici adiacenti, incluso $y$. Questo aggiornamento garantisce che:
$$ D(y) \le D(x) + w(x, y) = d(s, x) + w(x, y) $$

\noindent
Poiché $x$ e $y$ sono vertici consecutivi sul percorso minimo $P$ (che è un percorso minimo globale), anche il sotto-percorso da $s$ a $y$ deve essere un percorso minimo. Pertanto:
$$ d(s, y) = d(s, x) + w(x, y) $$

\noindent
Combinando le due equazioni precedenti, otteniamo che l'etichetta di $y$ è già corretta al momento dell'estrazione di $z$:
$$ D(y) = d(s, y) $$

\noindent
Ora arriviamo al cuore della dimostrazione logica.
L'algoritmo di Dijkstra è \textit{greedy}: seleziona sempre il vertice fuori da $C$ con l'etichetta $D$ più piccola. In questo momento, sia $z$ che $y$ sono fuori da $C$ (sono nella coda $Q$), ma l'algoritmo ha scelto di estrarre $z$. Questo implica necessariamente che:
$$ D(z) \le D(y) $$

\noindent
Qui entra in gioco l'ipotesi fondamentale che non esistano archi con peso negativo. La distanza reale da $s$ a $z$ può essere scomposta nella distanza da $s$ a $y$ più la distanza rimanente da $y$ a $z$:
$$ d(s, z) = d(s, y) + d(y, z) $$

\noindent
Poiché i pesi sono non negativi, allora si ha che $d(y, z) \ge 0$. Di conseguenza:
$$ d(s, y) \le d(s, z) $$

\noindent
Mettiamo insieme la catena di disuguaglianze:
\begin{alignat*}{4}
    D(z)    &\le{}& D(y)    &\quad& \text{(} &\text{per la scelta greedy dell'algoritmo)} \\
    D(y)    &={} & d(s,y)  &\quad& \text{(} &\text{perché l'etichetta di $y$ è già corretta al momento dell'estrazione di $z$)} \\
    d(s,y)  &\le{}& d(s,z) &\quad& \text{(} &\text{perché i pesi sono non-negativi)}
\end{alignat*}

\noindent
Unendo tutto:
$$ D(z) \le d(s, z) $$

\noindent
Ma questo contraddice la nostra ipotesi iniziale per assurdo, secondo cui $D(z) > d(s, z)$.

\vspace{1\baselineskip}
\noindent
La contraddizione dimostra che non può esistere un "primo vertice sbagliato" $z$. Pertanto, per ogni vertice $v$ aggiunto a $C$, l'etichetta $D(v)$ rappresenta sempre la distanza minima corretta $d(s, v)$.



\subsection{Analisi della complessità}
Indichiamo con $n$ e $m$ rispettivamente il numero di vertici e di archi del grafo di input $G$. Assumiamo che i pesi degli archi possano essere sommati e confrontati in tempo costante. 

Per analizzare il tempo di esecuzione dell'algoritmo di Dijkstra dobbiamo innanzitutto specificare come viene rappresentato il grafo $G$ e come viene implementata la coda di priorità $Q$. Assumiamo di rappresentare il grafo $G$ tramite \emph{Adjacency List} o \emph{Adjacency Map}. In questo modo possiamo attraversare i vertici adiacenti a $u$ durante la fase di rilassamento in tempo proporzionale al loro numero. Pertanto, il tempo speso nella gestione del ciclo annidato \texttt{for}, e il numero di iterazioni di quel ciclo, è dato da:

$$
\sum_{u \text{ in } V} outdeg(u)
$$

\noindent
che è $O(m)$. Il ciclo esterno \texttt{while} viene eseguito $O(n)$ volte, poiché in ogni iterazione viene aggiunto un nuovo vertice alla nuvola. 

Tuttavia, per completare l'analisi dell'algoritmo, dobbiamo considerare anche l'implementazione della coda di priorità $Q$. La coda a priorità ad ogni iterazione dell'algoritmo contiene gli elementi che non sono ancora presenti nella soluzione $C$, per cui inizialmente vengono effettuate $n$ operazioni di inserimento in $Q$ (soluzione ancora vuota); queste sono le uniche operazioni di inserimento, quindi la dimensione massima della coda è $n$. In ciascuna delle $n$ iterazioni del ciclo \texttt{while}, viene effettuata una chiamata a \texttt{remove\_min()} per estrarre il vertice $u$ con l'etichetta $D$ più piccola da $Q$. Successivamente, per ogni vicino $v$ di $u$, viene eseguito un rilassamento dell'arco, che potrebbe comportare un aggiornamento della chiave di $v$ nella coda \footnote
{
    All'interno della coda a priorità $Q$, ciascun elemento ha per chiave il valore dell'etichetta $D(v)$ del vertice corrispondente $v$, e il valore associato è il vertice $v$ stesso. 
}. Pertanto, è necessario implementare una \emph{Adaptable Priority Queue}, in cui la chiave di un vertice $v$ può essere modificata utilizzando il metodo \texttt{update(l,k)}, dove $l$ è il \emph{locator} per l'elemento della coda associato al vertice $v$. Nel peggiore dei casi, potrebbe esserci un aggiornamento per ogni arco del grafo. Complessivamente, il tempo di esecuzione dell'algoritmo di Dijkstra è limitato dalla somma delle seguenti operazioni:
\begin{itemize}[nosep]
    \item $n$ operazioni di inserimento in $Q$.
    \item $n$ chiamate al metodo \texttt{remove\_min()} su $Q$.
    \item $m$ chiamate al metodo \texttt{update()} su $Q$.
\end{itemize}

\noindent
Se $Q$ è una Adaptable Priority Queue implementata come heap, allora ciascuna delle operazioni sopra descritte viene eseguita in tempo $O(\log n)$, e quindi il tempo complessivo per l'algoritmo di Dijkstra è $O((n + m) \log n)$ (nel caso di un grafo connesso si ha $O(m \log n)$, dal momento che $m \geq n - 1$). Si noti che se si desidera esprimere il tempo di esecuzione come funzione di $n$ soltanto, allora nel caso peggiore è $O(n^2 \log n)$.

\vspace{1\baselineskip}
\noindent
È possibile implementare la coda a priorità $Q$ utilizzando un'\emph{array non ordinato}. In questo caso, l'operazione di estrazione del minimo richiede $O(n)$ tempo, mentre l'aggiornamento delle chiavi può essere eseguito in tempo costante. Pertanto, il tempo totale di esecuzione dell'algoritmo di Dijkstra diventa $O(n^2 + m)$, che si semplifica a $O(n^2)$ per grafi semplici.

\subsubsection{Confronto tra le due implementazioni - Adaptable Priority Queue vs Array non ordinato}
Come abbiamo visto, abbiamo due possibili scelte per implementare la coda a priorità $Q$ nell'algoritmo di Dijkstra: una implementazione basata su heap, che porta a un tempo di esecuzione di $O((n + m) \log n)$, e una implementazione basata su array non ordinato, che porta a un tempo di esecuzione di $O(n^2)$. Dal momento che entrambe le implementazioni hanno lo stesso livello di complessità (a livello di codice), la scelta tra le due dipende principalmente dalla densità del grafo di input.

In generale, si preferisce l'implementazione basata su heap quando il grafo è sparso (ovvero, quando il numero di archi è abbastanza piccolo, e si ha $m < n^2 / \log n$). Si preferisce invece l'implementazione basata su array non ordinato quando il grafo è denso (ovvero, quando il numero di archi è vicino al massimo possibile, e si ha $m > n^2 / \log n$).


\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1} 
    
    % Definizione colonne:
    % | l | -> Prima colonna: larghezza minima necessaria (adatta al contenuto)
    % X |   -> Seconda colonna: riempie lo spazio disponibile
    % X |   -> Terza colonna: riempie lo spazio disponibile
    \begin{tabularx}{\textwidth}{| l | X | X |}
        \hline
        % Prima riga: applico il grassetto manualmente qui
        \textbf{Implementation of the Priority Queue} & \textbf{Complexity} & \textbf{Usable for} \\
        \hline
        
        Binary Heap     & $O(m \log n)$     & $m < n^2 / \log n$ \\
        \hline
        
        Unordered Array & $O(n^2)$          & $m > n^2 / \log n$ \\
        \hline
        
        Fibonacci Heap  & $O(m + n \log n)$ & always \\
        \hline
    \end{tabularx}
    \caption{Si noti che esiste un'implementazione avanzata della coda a priorità, nota come \textbf{Fibonacci heap}, che può essere utilizzata per implementare l'algoritmo di Dijkstra in tempo $O(m + n \log n)$. Si tratta di un'implementazione più complessa, ma che offre prestazioni migliori in tutti i casi, utile in applicazioni specifiche che richiedono un'elevata efficienza.}
    \label{tab:dijkstra_implementations_comparison}
\end{table}


\clearpage
\subsection{Implementazione Python dell'algoritmo di Dijkstra}
\vspace{2\baselineskip}
\begin{lstlisting}[
    language=Python,
    caption={Implementazione Python dell'algoritmo di Dijkstra per il calcolo delle distanze degli shortest path da una singola sorgente a tutti i vertici raggiungibili in un grafo pesato. Si assume che il metodo \texttt{e.element()} per un arco \texttt{e} rappresenti il peso di quell'arco.},
    captionpos=b,
    label={lst:Dijkstra},
]
def shortest_path_lengths(g, src):
    """Compute shortest-path distances from src to reachable vertices of g.

    Graph g can be undirected or directed, but must be weighted such that
    e.element() returns a numeric weight for each edge e.

    Return dictionary mapping each reachable vertex to its distance from src.
    """
    d = { }                   # d[v] is upper bound from s to v
    cloud = { }               # map reachable v to its d[v] value
    pq = AdaptableHeapPriorityQueue( )  # vertex v will have key d[v]
    pqlocator = { }           # map from vertex to its pq locator

    # for each vertex v of g, add an entry to the priority queue, with
    # the source having distance 0 and all others having infinite distance
    for v in g.vertices( ):
        if v is src:
            d[v] = 0
        else:
            d[v] = float( inf )      # syntax for positive infinity
        pqlocator[v] = pq.add(d[v], v)  # save locator for future updates

    while not pq.is_empty( ):
        key, u = pq.remove_min()
        cloud[u] = key              # its correct d[u] value
        del pqlocator[u]            # u is no longer in pq
        for e in g.incident_edges(u):   # outgoing edges (u,v)
            v = e.opposite(u)
            if v not in cloud:
                # perform relaxation step on edge (u,v)
                wgt = e.element( )
                if d[u] + wgt < d[v]:   # better path to v?
                    d[v] = d[u] + wgt   # update the distance
                    pq.update(pqlocator[v], d[v], v)  # update the pq entry

    return cloud                 # only includes reachable vertices
\end{lstlisting}

\clearpage
\noindent
Il codice visto finora per l'algoritmo di Dijkstra calcola correttamente le distanze minime da una sorgente $s$ a tutti i vertici raggiungibili in un grafo pesato. Tuttavia, in molte applicazioni pratiche, è altrettanto importante poter ricostruire i percorsi effettivi che costituiscono gli shortest path, non solo le loro lunghezze. 

La collezione di tutti gli shortest path che partono dalla sorgente $s$ può essere rappresentata in modo compatto da quella che è nota come \textbf{shortest-path tree}. I percorsi formano un albero radicato perché se uno shortest path da $s$ a $v$ passa attraverso un vertice intermedio $u$, deve necessariamente iniziare con uno shortest path da $s$ a $u$.
In questa sezione, dimostriamo che lo shortest-path tree radicato in $s$ può essere ricostruito in tempo $O(n+m)$, dato l'insieme dei valori $d[v]$ prodotti dall'algoritmo di Dijkstra utilizzando $s$ come sorgente. Come abbiamo fatto quando abbiamo rappresentato gli alberi DFS e BFS, mapperemo ogni vertice $v \neq s$ a un genitore $u$ (eventualmente può essere $u = s$), tale che $u$ è il vertice immediatamente precedente di $v$ su uno shortest path da $s$ a $v$. Se $u$ è il vertice immediatamente precedente di $v$ sullo shortest path da $s$ a $v$, deve essere che:

$$ d[u] + w(u,v) = d[v] $$

\noindent
Viceversa, se l'equazione sopra è soddisfatta, allora lo shortest path da $s$ a $u$, seguito dall'arco $(u,v)$ è uno shortest path per $v$.
La nostra implementazione presentata di seguito ricostruisce l'albero basandosi su questa logica, testando tutti gli archi entranti per ogni vertice $v$, cercando un $(u,v)$ che soddisfi l'equazione chiave. Il tempo di esecuzione è $O(n+m)$, poiché consideriamo ogni vertice e tutti gli archi entranti a quei vertici.


\vspace{2\baselineskip}
\begin{lstlisting}[
    language=Python,
    caption={Funzione in Python che ricostruisce lo shortest-path tree radicato in un vertice sorgente \texttt{s}, dato il grafo \texttt{g} e la mappa delle distanze \texttt{d} calcolate dall'algoritmo di Dijkstra. La funzione restituisce un dizionario che mappa ogni vertice raggiungibile (diverso da \texttt{s}) all'arco utilizzato per raggiungerlo dal suo genitore nello shortest-path tree.},
    captionpos=b,
    label={lst:Dijkstra_shortest_path_tree},
]
def shortes_path_tree(g, s, d):
    """Reconstruct shortest-path tree rooted at vertex s, given distance map d.

    Return tree as a map from each reachable vertex v (other than s) to the
    edge e=(u,v) that is used to reach v from its parent u in the tree.
    """
    tree = { }
    for v in d:
        if v is not s:
            for e in g.incident_edges(v, False): # consider INCOMING edges
                u = e.opposite(v)
                wgt = e.element( )
                if d[v] == d[u] + wgt:
                    tree[v] = e # edge e is used to reach v
    return tree
\end{lstlisting}


\clearpage
\section{Algoritmo di Bellman-Ford}
L'algoritmo di Dijkstra non garantisce la correttezza della soluzione in presenza di archi con peso negativo all'interno del grafo. Questa limitazione deriva dalla natura \emph{greedy} dell'algoritmo, il quale costruisce l'insieme dei cammini minimi selezionando iterativamente i vertici in ordine crescente di distanza.

Il fondamento logico di Dijkstra risiede nell'assunzione che, estendendo un percorso, la distanza totale non possa mai diminuire. Quando un vertice $u$ viene estratto dalla coda di priorità e aggiunto all'insieme dei nodi visitati (soluzione parziale), la sua distanza $D[u]$ è considerata \textbf{definitiva}. Se tutti i pesi sono non negativi ($w \ge 0$), questa assunzione è corretta poiché qualsiasi percorso alternativo che passi per nodi non ancora visitati avrà necessariamente una lunghezza maggiore o uguale a quella attuale.

Tuttavia, l'introduzione di archi con peso negativo invalida questa proprietà. Un arco negativo potrebbe rivelare un percorso "più lungo" in termini di numero di archi, ma con un costo totale inferiore, che passa attraverso nodi che l'algoritmo ha già scartato o finalizzato.
Consideriamo il seguente scenario critico, in riferimento alla Figura \ref{fig:negative_weight}:
\begin{itemize}
    \item Un vertice $C$ viene inserito nella soluzione definitiva con una stima di distanza $D(C) = 5$. Per l'algoritmo, questo valore è ottimale e non verrà più modificato.
    \item Successivamente, l'algoritmo esplora un vertice $F$ e tenta di rilassare l'arco $(F, C)$. Se questo arco ha un peso fortemente negativo, potrebbe esistere un percorso che raggiunge $C$ passando per $F$ con un costo totale inferiore a $D(C)$ precedentemente calcolato (nel nostro caso $1 < 5$).
    \item Poiché l'algoritmo di Dijkstra non riconsidera i vertici già inseriti nell'insieme della soluzione (in quanto l'approccio greedy non consente backtracking), esso ignorerà questo aggiornamento, mantenendo erroneamente $D(C)=5$ invece del valore corretto $1$.
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{immagini/ShortestPath/negative_weight.png}
    \caption{Esempio di grafo con arco a peso negativo che causa un errore nell'algoritmo di Dijkstra.}
    \label{fig:negative_weight}
\end{figure}

\noindent
Per gestire grafi con pesi negativi (purché privi di cicli negativi\footnote{
    Un \textbf{ciclo negativo} è un ciclo (una sequenza chiusa di nodi) il cui peso totale (la somma dei pesi degli archi che lo compongono) è negativo. In presenza di cicli negativi, il concetto di shortest path diventa indefinito, poiché si potrebbe continuare a percorrere il ciclo negativo per ridurre indefinitamente la lunghezza del percorso.
}), è necessario ricorrere ad algoritmi alternativi come \textbf{l'algoritmo di Bellman-Ford}.

\clearpage
\section*{Gestione degli Archi Negativi: L'approccio di Bellman-Ford}
Per risolvere il problema dei cammini minimi in presenza di archi negativi (assumendo per semplicità un grafo orientato), è necessario abbandonare la logica \emph{greedy} di selezione dei vertici in favore di un approccio basato sul rilassamento iterativo globale. Questo metodo si fonda sulle proprietà descritte \textbf{dal Teorema di Bellman}.

\subsection*{Il Teorema di Bellman}
Il principio fondamentale afferma che per ogni arco $(u, v)$ nel grafo, la distanza calcolata verso $v$ deve soddisfare la disuguaglianza:
\[
d(v) \le d(u) + w(u, v) 
\]
Specificamente, un arco $(u, v)$ appartiene all'albero dei cammini minimi (Shortest Path Tree, $T_s$) se e solo se la condizione di uguaglianza è soddisfatta:
\[
d(v) = d(u) + w(u, v)
\]

\subsection*{Funzionamento dell'Algoritmo}
A differenza di Dijkstra, non consideriamo l'ordine in cui i vertici vengono aggiunti all'albero. L'algoritmo procede invece per iterazioni fisse:
\begin{itemize}
    \item \textbf{Strategia Globale:} Ad ogni iterazione, l'algoritmo tenta di rilassare \textbf{tutti} gli archi presenti nel grafo, cercando sistematicamente percorsi più brevi.
    \item \textbf{Numero di Passi:} L'algoritmo esegue esattamente $n-1$ cicli (dove $n$ è il numero dei vertici).
    \item \textbf{Significato del passo $k$:} Alla $k$-esima iterazione, l'algoritmo garantisce di aver trovato i cammini minimi che sono costituiti da esattamente (o al più) $k$ archi.
\end{itemize}

\noindent
Poiché un cammino semplice (cioè senza cicli) in un grafo di $n$ nodi può contenere al massimo $n-1$ archi, dopo $n-1$ iterazioni siamo certi che tutte le distanze siano state propagate correttamente attraverso la rete, anche in presenza di pesi negativi.


\vspace{1\baselineskip}
\noindent
Questo algoritmo è particolarmente interessante in quanto è alla base dei protocolli di routing distance-vector, essenziali per instradare i dati su Internet. Sfruttando la logica di Bellman-Ford, i router non hanno bisogno di conoscere l'intera mappa della rete: scambiandosi semplicemente le stime delle distanze con i vicini e applicando iterativamente il rilassamento, riescono a calcolare il percorso ottimale verso qualsiasi destinazione in modo distribuito e dinamico.


\clearpage
\subsection*{Pseudocodice dell'Algoritmo}
\hrule 
\vspace{1\baselineskip}
\begin{verbatim}
1.  Algorithm BellmanFord(G, s)
2.      for all v in G.vertices() do
3.          if v = s then
4.              d(v) = 0
5.          else
6.              d(v) = infinity
7.      for i = 1 to n - 1 do
8.          for each e in G.edges() do
9.              // relax edge e
10.             u = G.origin(e)
11.             v = G.opposite(u, e)
12.             r = d(u) + weight(e)
13.             if r < d(v) then
14.                 d(v) = r
\end{verbatim}
\hrule 
\vspace{1\baselineskip}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/ShortestPath/ex_BF1.png}
    \label{fig:ex_BF1}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/ShortestPath/ex_BF2.png}
    \caption{Esempio di esecuzione dell'algoritmo di Bellman-Ford. }
    \label{fig:ex_BF2}
\end{figure}


\clearpage
\subsection*{Analisi della complessità}
L'algoritmo di Bellman-Ford ha una complessità temporale di $O(n \cdot m)$, dove $n$ è il numero di vertici e $m$ è il numero di archi nel grafo. Questo deriva dal fatto che l'algoritmo esegue $n-1$ iterazioni e in ciascuna iterazione rilassa tutti gli $m$ archi del grafo. Pertanto, il tempo totale di esecuzione è proporzionale al prodotto del numero di vertici e del numero di archi, risultando in $O(n \cdot m)$. Questo rende l'algoritmo di Bellman-Ford meno efficiente rispetto all'algoritmo di Dijkstra per grafi con pesi non negativi, che può essere eseguito in tempo $O((n + m) \log n)$ utilizzando una coda di priorità basata su heap. Tuttavia, Bellman-Ford è l'unica alternativa quando si lavora con grafi che possono contenere archi con pesi negativi, purché non vi siano cicli negativi.


\section{Shortest Paths in un DAG}
Dopo aver analizzato l'algoritmo di Dijkstra (ottimo per grafi con pesi non negativi) e l'algoritmo di Bellman-Ford (necessario in presenza di pesi negativi), esaminiamo ora un caso particolare ma molto frequente: il calcolo dei cammini minimi in un \textbf{DAG} (\textit{Directed Acyclic Graph}).

\subsubsection{Richiamo: Cos'è un DAG}
Come definito in precedenza, un DAG è un grafo diretto che non contiene cicli. Questo significa che partendo da un qualsiasi nodo $u$ e seguendo gli archi orientati, è impossibile ritornare su $u$. Questa proprietà di "aciclicità" è fondamentale perché implica l'esistenza di un \textbf{ordinamento topologico} (Topological Sort).

\subsubsection{Perché un approccio specifico?}
Sebbene l'algoritmo di Bellman-Ford funzioni correttamente sui DAG (poiché non avendo cicli, non possono avere cicli negativi), esso ha una complessità di $O(n \cdot m)$. L'algoritmo di Dijkstra, d'altra parte, non supporta archi con peso negativo.

L'algoritmo specifico per i DAG supera entrambi i limiti sfruttando la struttura topologica del grafo:
\begin{itemize}
    \item \textbf{Gestisce pesi negativi:} A differenza di Dijkstra, funziona perfettamente anche se gli archi hanno peso negativo. Essendo il grafo aciclico, non esiste il rischio di "cicli negativi" che porterebbero il costo a $-\infty$.
    \item \textbf{Efficienza superiore:} È molto più veloce di Bellman-Ford e persino di Dijkstra, poiché non richiede strutture dati ausiliarie complesse (come le code di priorità) e visita ogni arco esattamente una volta.
\end{itemize}

\clearpage
\subsubsection{L'Algoritmo: Rilassamento in Ordine Topologico}
L'idea chiave è la seguente: se visitiamo i nodi in ordine topologico, quando arriviamo a elaborare un nodo $u$, abbiamo la garanzia che il cammino minimo per raggiungerlo, $D(u)$, sia già stato calcolato definitivamente. Non esistono archi che tornano "indietro" da nodi successivi che potrebbero aggiornare e migliorare $D(u)$.

L'algoritmo procede in due fasi:
\begin{enumerate}
    \item \textbf{Ordinamento Topologico:} Si ordinano linearmente i vertici del grafo in modo che per ogni arco $(u, v)$, $u$ appaia prima di $v$ nella sequenza.
    \item \textbf{Rilassamento Lineare:} Si inizializzano le distanze (0 per la sorgente $s$, $\infty$ per gli altri). Si scorrono i nodi $u$ secondo l'ordine topologico e, per ciascuno, si rilassano tutti gli archi uscenti.
\end{enumerate}

\vspace{1\baselineskip}
\hrule
\begin{verbatim}
1.  Algorithm DagDistances(G, s)
2.      for all v in G.vertices() do
3.          if v = s then
4.              d(v) = 0
5.          else
6.              d(v) = infinity
7.      // build a topological sort of the vertices
8.      for u = 1 to n - 1 do // in topological order
9.          for each e in G.outEdges(u) do
10.             // relax edge e
11.             v = G.opposite(u, e)
12.             r = d(u) + weight(e)
13.             if r < d(v) then
14.                 d(v) = r            
\end{verbatim}
\hrule 
\vspace{1\baselineskip}


\subsubsection{Analisi della Complessità}
La complessità dell'algoritmo è determinata da due passaggi:
\begin{itemize}
    \item L'ordinamento topologico può essere eseguito in tempo $O(n + m)$.
    \item Il ciclo di rilassamento visita ogni vertice una volta e ogni arco esattamente una volta, impiegando anch'esso $O(n + m)$.
\end{itemize}

\noindent
Pertanto, la complessità temporale totale è:
\[ O(n + m) \]
Questo lo rende un algoritmo a tempo lineare, la soluzione asintoticamente ottima per questo problema.


\clearpage
\section{All-Pairs Shortest Paths}
Fino ad ora ci siamo concentrati sul problema \textit{Single-Source Shortest Path}, ovvero trovare i cammini minimi da una singola sorgente verso tutti gli altri nodi.
Come abbiamo già visto, esiste una generalizzazione del problema: il calcolo dei cammini minimi \textbf{tra ogni coppia di vertici} del grafo.
Anche in questo caso, gli archi possono avere pesi negativi, purché non esistano cicli negativi. 


\subsubsection{Approccio Ingenuo: Iterare SSSP}
Una prima soluzione intuitiva consiste nell'eseguire un algoritmo \textit{Single-Source} per ogni vertice del grafo, considerandolo di volta in volta come sorgente.

\begin{itemize}
    \item Se usiamo \textbf{Dijkstra}: Funziona solo se i pesi sono non negativi. La complessità sarebbe $O(n \cdot (m + n \log n)) = O(nm + n^2 \log n)$.
    \item Se usiamo \textbf{Bellman-Ford}: Necessario se ci sono pesi negativi. La complessità sarebbe $O(n (n \cdot m)) = O(n^2 m)$.
\end{itemize}
Se il grafo è denso (dove $m \approx n^2$), la complessità dell'approccio ingenuo con Bellman-Ford diventa $O(n^4)$, che è spesso inaccettabile.

\subsubsection{Soluzione Efficiente: Floyd-Warshall}
Per migliorare l'efficienza, Floyd e Warshall hanno proposto un algoritmo basato sulla \textbf{Programmazione Dinamica}. L'idea è simile a quella usata per calcolare la chiusura transitiva di un grafo.

Il calcolo si basa sulla seguente ricorrenza per $SP(i, j, k)$, che rappresenta il cammino minimo da $i$ a $j$ considerando solo i primi $k$ vertici come intermedi:

\begin{enumerate}
    \item \textbf{Caso Base ($k=0$):}
    \[
    SP(i, j, 0) = 
    \begin{cases} 
    w(i, j) & \text{se } (i, j) \in E \\
    \infty & \text{altrimenti}
    \end{cases}
    \]
    
    \item \textbf{Passo Ricorsivo:}
    La distanza minima considerando il $k$-esimo vertice è il minimo tra non usarlo (restare al passo $k-1$) e usarlo come ponte:
    \[
    SP(i, j, k) = \min \{ SP(i, j, k-1), \quad SP(i, k, k-1) + SP(k, j, k-1) \}
    \]
\end{enumerate}

\noindent
La complessità temporale di questo algoritmo è:
\[ O(n^3) \]

\section{Riepilogo Finale: Algoritmi di Shortest Path}
Ecco una sintesi degli algoritmi trattati in questo capitolo per scegliere quello più adatto in base al contesto:

\vspace{1\baselineskip}
\begin{table}[h!]
    \centering
    \caption{Confronto delle complessità degli algoritmi di Shortest Path.} 
    \label{tab:confronto_algoritmi}
    \vspace{0.2cm} % Un po' di spazio tra il titolo e la tabella
    \begin{tabular}{|l|l|l|l|}
        \hline
        \textbf{Algoritmo} & \textbf{Tipo} & \textbf{Vincoli} & \textbf{Complessità} \\ \hline
        \textbf{BFS} & SSSP & Grafo non pesato & $O(n + m)$ \\ \hline
        \textbf{Dijkstra} & SSSP & Pesi non negativi ($w \ge 0$) & $O(m + n \log n)$ \\ \hline
        \textbf{Bellman-Ford} & SSSP & Pesi generici (no cicli neg.) & $O(n \cdot m)$ \\ \hline
        \textbf{DAG-SP} & SSSP & DAG, Pesi generici (no cicli neg.) & $O(n + m)$ \\ \hline
        \textbf{Floyd-Warshall} & All-Pairs & Pesi generici (no cicli neg.) & $O(n^3)$ \\ \hline
    \end{tabular}
\end{table}