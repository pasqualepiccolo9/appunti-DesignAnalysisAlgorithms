\chapter{Greedy Algorithms}
\label{cap:Greedy}

L'\emph{approccio Greedy} è un paradigma di progettazione di algoritmi utilizzato per risolvere \textbf{problemi di ottimizzazione}. Un algoritmo greedy funziona bene quando una soluzione ottimale può essere raggiunta attraverso una \emph{serie di scelte locali ottimali}. A partire da una \emph{configurazione iniziale} (soluzione parziale), l'algoritmo effettua una scelta (la migliore possibile localmente) in una classe di possibili opzioni, e ripete lo stesso procedimento aggiornando di volta in volta la configurazione corrente, fino a raggiungere una soluzione completa.  

In particolare, non possiamo utilizzare questo approccio per tutti i problemi di ottimizzazioni. Diciamo che un problema di ottimizzazione ammette una \textbf{soluzione greedy} se il problema soddisfa la proprietà:
\begin{itemize}
    \item \textbf{Greedy-choice property}: la soluzione completa ottimale può sempre essere raggiunta effettuando una serie di progressi, che rappresentano delle scelte locali ottimali, a partire da una configurazione iniziale.
\end{itemize}

\noindent
Ad ogni passo, la "scelta" deve essere:
\begin{itemize}
    \item \textbf{Realizzabile}: deve soddisfare i vincoli dettati dal problema.
    \item \textbf{Localmente ottima}: deve essere la scelta migliore tra tutte le scelte possibili (e realizzabili) in quel momento.
    \begin{itemize}[nosep]
        \item Non è necessario che questa scelta sia ottima rispetto alla soluzione globale.
        \item Da notare che non è sempre detto che effettuare scelte localmente ottimali porti ad una soluzione globale ottimale. In questi casi, l'algoritmo greedy non funziona correttamente.
    \end{itemize}
    \item \textbf{Irrevocabile}: una volta effettuata una scelta, questa non può essere modificata in futuro.
\end{itemize}

\noindent
Un algoritmo greedy costruisce una soluzione in piccoli passi successivi, scegliendo ad ogni passo una decisione che riguarda esclusivamente la configurazione corrente. Spesso si possono progettare molti algoritmi greedy diversi per lo stesso problema, ognuno dei quali ottimizza localmente e in modo incrementale qualche misura diversa nel suo cammino verso una soluzione. 
È facile inventare algoritmi greedy per quasi tutti i problemi; trovare i casi in cui funzionano bene, e dimostrare che effettivamente funzionano bene, è la sfida interessante.



\clearpage
\section{Un modello generale}
Indichiamo con $S$ la soluzione parziale corrente, e con $P$ il sotto problema che rimane da risolvere. Inizialmente, $S$ è vuota e $P$ coincide con il problema originale. Ad ogni passo, l'algoritmo greedy:

\vspace{1\baselineskip}
\hrule

\begin{verbatim}
1. Generate all candidate choices as list L for current sub-problem P.
2. While (L is not empty or other finish condition is not met)
3.     Compute the feasible value of each choice in L;
4.     Modify S and P by taking choice with the highest feasible value;
5.     Update L according to S and P;
6. Endwhile
7. Return the resulting complete solution.
\end{verbatim}

\hrule 
\vspace{1\baselineskip}

\noindent
Sia $A$ l'insieme di tutti i possibili elementi del problema. Ad ogni step, l'algoritmo greedy mantiene una partizione $<X, Y, W>$ di $A$, dove:
\begin{itemize}[nosep]
    \item $X$: insieme degli elementi selezionati fino a quel momento (soluzione parziale corrente).
    \item $Y$: insieme degli elementi valutati ma non ancora selezionati.
    \item $W$: insieme degli elementi non ancora valutati.
\end{itemize}
Inizialmente, $W = A$ e $X = Y = \emptyset$. Alla fine dell'algoritmo, $X$ conterrà la soluzione completa, $Y = A/X$ contiene tutti gli elementi di $A$ che non sono stati selezionati, e $W = \emptyset$.

\vspace{1\baselineskip}
\noindent
Gli algoritmi greedy sono spesso \textbf{estremamente intuitivi} al tal punto da rappresentare la soluzione più semplice e naturale per molti problemi. Sono anche molto \textbf{efficienti}, con complessità temporali che vanno da $O(n \log n)$ a $O(n)$, dove $n$ è la dimensione dell'input. Tuttavia, la loro efficienza dipende fortemente dalla natura del problema e dalla struttura dei dati utilizzati per implementare l'algoritmo.

La parte più complicata nella progettazione di un algoritmo greedy è la \textbf{dimostrazione della correttezza} dell'algoritmo, che spesso richiede tecniche di dimostrazione specifiche per ogni problema. Le tecniche più comuni sono:
\begin{itemize}
    \item \textbf{Greedy stays ahead}: si dimostra che, ad ogni passo dell'algoritmo, la soluzione parziale costruita dall'algoritmo greedy è almeno altrettanto buona quanto qualsiasi altra soluzione parziale possibile.
    \item \textbf{Exchange}: si dimostra che qualsiasi soluzione ottimale può essere trasformata nella soluzione prodotta dall'algoritmo greedy attraverso una serie di scambi di elementi, senza peggiorare la qualità della soluzione.
\end{itemize}


\clearpage
\section{Coin Change: The Cashier Algorithm}
Il problema del \emph{Coin Change} (cambio di monete) consiste nel trovare il numero minimo di monete necessarie per rappresentare un dato importo di denaro, utilizzando un insieme predefinito di tagli di monete. 

Il problema prende in input un importo R (in centesimi di euro) e richiede in output il numero minimo di monete necessarie per rappresentare tale importo, utilizzando solo, ad esempio, monete di taglio 2€, 1€, 50c, 20c, 10c, 5c, 2c e 1c. Si assume di avere a disposizione un numero illimitato di monete per ogni taglio.

\vspace{1\baselineskip}
\hrule

\begin{verbatim}
Sort coins denominations by value: c[1] > c[2] > ... > c[k] 
1. S = {}           // inizializza soluzione parziale (insieme vuoto)
2. while (x != 0) { // finché l'importo da cambiare non è zero
3.    let k be the largest integer such that c[k] <= x
4.    if (k = 0)
5.        return "no solution found"
6.    x = x - c[k]  // riduci l'importo rimanente
7.    S = {S, c[k]} // aggiungi c[k] alla soluzione
8. }
9. return S         // restituisci la soluzione completa
\end{verbatim}

\hrule 
\vspace{1\baselineskip}

\begin{lstlisting}
def coin_change(amount_rem):
    coin_combinations = [50, 20, 10, 5, 2, 1]   # Valori in centesimi
    result = []                  

    for coin in coin_combinations:
        coin_count = amount_rem // coin # Divisione intera per ottenere il numero massimo di monete di questo taglio
        result += [coin] * coin_count   # Aggiungi le monete alla soluzione
        amount_rem -= coin * coin_count # Aggiorna l'importo rimanente
        if amount_rem == 0:
            return result       # Restituisci la soluzione completa

    if amount_rem > 0:      # Se non e' stato possibile coprire l'importo
        return "No solution found"
\end{lstlisting}
\vspace{1\baselineskip}

\noindent
In generale, questo algoritmo restituisce sempre una soluzione (non necessariamente ottimale) se $c[k] = 1$ (cioè se esiste una moneta di taglio unitario). Tuttavia, l'algoritmo è ottimale solo per alcuni sistemi di monete specifici, come quello europeo, i cosiddetti \textbf{sistemi canonici}. 

Un insieme di monete è un \textbf{sistema canonico} se l'algoritmo del cassiere fornisce la soluzione ottimale per ogni importo $R$ da cambiare. In generale, non tutti i sistemi di monete sono canonici e determinare se un sistema di monete è canonico può essere un problema complesso.


\clearpage
\section{Scheduling}
Il problema dello \emph{scheduling} riguarda la pianificazione di un insieme di attività o compiti su risorse limitate, come macchine, lavoratori o tempo. L'obiettivo è ottimizzare l'uso delle risorse per massimizzare l'efficienza, minimizzare i tempi di completamento o soddisfare altre metriche di performance. Le attività possono avere vincoli di tempo, priorità diverse e requisiti specifici, rendendo il problema complesso e variegato.

In generale, dato un insieme di $n$ attività ognuna con un \emph{tempo di inizio} $s_i$ e un \emph{tempo di fine} $f_i$ (con $s_i < f_i$), si chiede di:

\begin{itemize}
    \item \textbf{Task Scheduling}: Minimizzare il numero di macchine (in generale risorse) necessarie per completare tutte le attività senza sovrapposizioni.
    \item \textbf{Interval Scheduling}: Massimizzare il numero di attività che possono essere completate senza sovrapposizioni, utilizzando una singola macchina.
\end{itemize}

\subsection{Task Scheduling}
In questa tipologia di problemi, abbiamo delle risorse identiche limitate (useremo il termine "macchine" per semplicità) e desideriamo assegnare un insieme di attività a queste macchine in modo tale che nessuna attività si sovrapponga temporalmente su una stessa macchina. L'obiettivo è minimizzare il numero di macchine utilizzate per completare tutte le attività.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.99\textwidth]{immagini/Greedy/ex_task_scheduling.png}
    \caption{(a) Una istanza del problema di Task Scheduling, in cui le risorse sono delle aule e le attività sono lezioni da assegnare. (b) Una soluzione in cui tutte le attività sono pianificate utilizzando tre risorse: ogni riga rappresenta un insieme di attività che possono essere tutte pianificate su una singola risorsa.}
    \label{ex:task_scheduling}
\end{figure}

\clearpage
\subsection*{Soluzione Greedy}
Una soluzione greedy molto intuitiva per questo problema consiste nell'ordinare le attività in ordine crescente di tempo di inizio. Successivamente, si itera attraverso l'elenco delle attività e si assegna ciascuna attività alla prima macchina disponibile che non abbia conflitti di orario con le attività già assegnate. Se nessuna macchina è disponibile, si aggiunge una nuova macchina.

\vspace{1\baselineskip}
\hrule

\begin{verbatim}
Sort intervals by starting time so that s[1] <= s[2] <= ... <= s[n]
1.  d = 0    // inizializza il numero di macchine
2.  for j = 1 to n {
3.      if (task j is compatible with some machine k)
4.          schedule task j on machine K
5.      else 
6.          allocate a new machine d + 1
7.          schedule task j on machine d + 1
8.          d = d + 1
9.  }
10. return d  // restituisci il numero di macchine utilizzate
\end{verbatim}

\hrule
\vspace{2\baselineskip}

\noindent
L'algoritmo ha complessità complessiva $O(n \log n)$, dominata
dall'ordinamento iniziale delle attività per tempo di inizio.

Nella fase di assegnazione si utilizza una \emph{Min-Priority Queue} (min-heap) contenente, per ciascuna macchina, il tempo di fine dell'ultimo task eseguito. In questo modo la radice del heap rappresenta sempre la macchina che si libera per prima.

Per ogni attività $j$, processata in ordine crescente di $s[j]$, è sufficiente confrontare $s[j]$ con il minimo del heap:

\begin{itemize}
    \item se $s[j] \ge f_{\min}$, la macchina si è liberata: si estrae il minimo e si inserisce il nuovo tempo di fine ($O(\log n)$);
    \item altrimenti, tutte le macchine sono occupate: si alloca una nuova macchina e si inserisce il suo tempo di fine nel heap
          ($O(\log n)$).
\end{itemize}

\noindent
Poiché ogni attività effettua al più un'estrazione e un'inserzione nel heap, la fase di scansione richiede $O(n \log n)$, in linea con il costo dell'ordinamento.


\clearpage
\subsection{Interval Scheduling}
In questa tipologia di problemi, abbiamo una singola macchina e desideriamo selezionare un sottoinsieme di attività da eseguire su questa macchina in modo tale che nessuna attività si sovrapponga con un'altra. L'obiettivo è massimizzare il numero di attività completate senza sovrapposizioni.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/Greedy/ex_interval_scheduling.jpg}
    \caption{Una istanza del problema di Interval Scheduling in cui si hanno 8 attività da collocare su una singola macchina.}
    \label{ex:interval_scheduling}
\end{figure}

\subsection*{Soluzione Greedy}
Nel cercare una soluzione ottimale per questo problema, possiamo considerare diverse strategie greedy intuitive, e selezionare la prima attività compatibile con quelle già scelte. Alcune possibili strategie includono:
\begin{itemize}
    \item Ordinamento per tempo di inizio crescente. Seleziono le attività compatibili in ordine di inizio $s[i]$. 
    \item \textbf{Ordinamento per tempo di fine crescente}. Seleziono le attività compatibili in ordine di fine $f[i]$.
    \item Ordinamento per durata crescente. Seleziono le attività compatibili in ordine di durata $f[i] - s[i]$.
    \item Ordinamento per numero di conflitti crescente. Per ogni attività $i$, conto il numero $c_i$ di altre attività che non sono compatibili con $i$ (cioè che si sovrappongono temporalmente con $i$). Seleziono le attività compatibili in ordine crescente di $c_i$.
\end{itemize}

\clearpage
\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/Greedy/interval_scheduling_counterex.png}
    \caption{Controesempi per le strategie greedy di Interval Scheduling basate su (a) tempo di inizio crescente, (b) durata crescente, (c) numero di conflitti crescente.}
    \label{ex:interval_scheduling_counterex}
\end{figure}

\noindent
Tra queste strategie, solo l'\textbf{ordinamento per tempo di fine crescente} garantisce una \textbf{soluzione ottimale} per il problema di \emph{Interval Scheduling}. Le altre strategie possono portare a soluzioni subottimali, come mostrato nei controesempi della Figura \ref{ex:interval_scheduling_counterex}. Dal momento che per confutare una strategia greedy è sufficiente trovare un singolo controesempio, possiamo concludere che l'unica strategia ottimale tra quelle elencate è quella basata sul tempo di fine crescente (ci siamo limitati a confutare le strategie mostrate in Figura \ref{ex:interval_scheduling_counterex}, ma per essere certi che la strategia basata sul tempo di fine crescente sia ottimale, sarebbe necessario dimostrarne la correttezza).

\vspace{1\baselineskip}
\hrule

\begin{verbatim}
Sort intervals by finishing time so that f[1] < f[2] < ... < f[n]
1.  n = s.length    // number of activities
2.  A = {a[1]}      // initialize solution with first activity
3.  k = 1           // index of last activity added to A
4.  for m=2 to n {
5.      if (s[m] >= f[k])
6.          A = {A, a[m]} // add activity a[m] to A
7.          k = m   // update index of last activity added to A
8.  }
9.  return A        // return the set of accepted activities 
\end{verbatim}

\hrule
\vspace{2\baselineskip}

\noindent
L'algoritmo ha complessità complessiva $O(n \log n)$, dominata
dall'ordinamento iniziale delle attività per tempo di fine. L'algoritmo sfrutta una semplice \emph{scansione lineare} delle attività ordinate, selezionando ogni volta la prima attività compatibile con l'ultima selezionata.


\clearpage
\section{Fractional Knapsack}
Il problema del \emph{Fractional Knapsack} consiste nel massimizzare il valore totale degli elementi inseriti in uno "zaino" (knapsack) con una capacità limitata, permettendo di prendere frazioni degli oggetti. 

Data una sequenza $S$ di $n$ elementi, ognuno dei quali ha un \textbf{peso} $p_i$ e un \textbf{valore} $v_i$ (entrambi interi positivi), e data una \textbf{capacità massima} $P$, l'obiettivo è selezionare una combinazione di elementi di $S$ (o frazioni di essi) in modo tale che il peso totale non superi $P$ e il valore totale sia massimizzato.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{immagini/Greedy/fractional_knapsack.png}
    \caption{Esempio di soluzione ottimale per un'istanza del problema di Fractional Knapsack con capacità $P = \text{10ml}$.}
    \label{ex:fractional_knapsack}
\end{figure}

\subsection*{Soluzione Greedy}
Una soluzione greedy per questo problema consiste nell'ordinare gli elementi in ordine decrescente di \emph{valore per unità di peso}, dato dal rapporto $v_i/p_i$. Questo valore rappresenta il \textbf{profitto} ottenuto per ogni unità di peso dell'elemento $i$.
Successivamente, gli elementi vengono inseriti per intero nello zaino in questo ordine, fino a quando non si raggiunge un elemento $j$, detto \emph{elemento critico}, che ha un peso maggiore della capacità massima residua dello zaino. A questo punto, si inserisce solo la frazione di $j$ che può essere contenuta nello zaino, e l'algoritmo termina, restituendo la soluzione completa. 



\clearpage
\hrule
\begin{verbatim}
Input:  Set S of items i with weight pi and value vi all positive
        Knapsack capacity P>0
Output: Amount xi of i that maximizes the total benefit without 
        exceeding in the capacity

1.  FractionalKnapsack(S, P):
2.      for each i in S do
3.          xi = 0              // initially the knapsack is empty
4.          bi = vi/wi          // the benefit of item i
5.      p = P       // remaining capacity in knapsack (initially p = P)
6.      while p > 0 do
7.          remove from S an item of maximal benefit    // greedy choice
8.          xi = min(pi,  p)    // can't carry more than p
9.          p -= xi             // update remaining capacity

\end{verbatim}

\hrule
\vspace{1\baselineskip}

\noindent
L'algoritmo ha una complessità complessiva di $O(n \log n)$, dominata dall'ordinamento iniziale degli elementi in base al loro valore per unità di peso. La fase di selezione degli elementi richiede invece $O(n)$, poiché ogni elemento viene considerato una sola volta.


\vspace{3\baselineskip}
\noindent
Come già accennato, progettare un algoritmo greedy è spesso semplice ed intuitivo; d'altra parte bisogna essere consapevoli che non tutti i problemi di ottimizzazione ammettono una soluzione \emph{ottimale} basata su questo approccio.

Ad esempio, il problema del \emph{0-1 Knapsack}, in cui gli elementi non possono essere frazionati (cioè devono essere presi per intero o lasciati fuori dallo zaino), non ammette una soluzione ottimale basata su un algoritmo greedy. In questo caso, è necessario utilizzare tecniche più complesse, come la programmazione dinamica, per trovare la soluzione ottimale.

I casi in cui un algoritmo greedy fallisce nel trovare la soluzione ottimale sono spesso caratterizzati dalla presenza di \textbf{scelte locali che non portano a una soluzione globale ottimale}, e trovare delle particolari istanze del problema che dimostrino questo fatto non è sempre banale. Inoltre, dimostrare che un algoritmo greedy è effettivamente ottimale per un dato problema può richiedere tecniche di dimostrazione specifiche e non sempre intuitive, anche se spesso si basano su idee semplici come \emph{greedy stays ahead} e \emph{exchange} già menzionate in precedenza.



\clearpage
\section{Text Compression}
La \emph{compressione del testo} è un'operazione che mira a ridurre la quantità di spazio necessario per memorizzare o trasmettere dati testuali. Questo processo è fondamentale in molte applicazioni, come l'archiviazione di file, la trasmissione di dati su reti a larghezza di banda limitata e l'ottimizzazione delle prestazioni dei sistemi informatici. Una tecnica comune per la compressione del testo è l'uso di algoritmi di codifica che sfruttano la ridondanza nei dati per rappresentarli in modo più efficiente.

Data una stringa $X$, vogliamo codificare in modo efficiente $X$ in una stringa binaria $Y$ più piccola (usando solo i caratteri 0 e 1). Uno dei modi possibili per risolvere questo problema è utilizzare la \textbf{codifica di Huffman} (Huffman code). 

\subsection{Codifica di Huffman}
Questa soluzione si basa sul calcolo della frequenza $f_c$ di ogni carattere $c$ che compare in $X$. L'idea è di assegnare codici binari più brevi ai caratteri che appaiono più frequentemente e codici più lunghi ai caratteri meno frequenti. In questo modo, la lunghezza totale della stringa codificata $Y$ sarà minimizzata.

\begin{itemize}[nosep]
    \item Una \textbf{code map} (mappa di codifica) associa ad ogni carattere un corrispondente codice binario, detto \emph{code-word}.
    \item Un \textbf{prefix code} (codice prefisso) è un sistema di codifica binario in cui nessuna parola di codice è il prefisso di un'altra parola di codice. Questo garantisce che la decodifica sia univoca e non ambigua.
    \item Un codice prefisso può essere rappresentato da un \textbf{encoding tree} (albero di codifica).
    \item Un \emph{encoding tree} è un albero binario in cui:
    \begin{itemize}[nosep]
        \item Ogni foglia rappresenta un carattere.
        \item La code-word associata ad un carrattere è data dal percorso dalla radice alla foglia corrispondente che memorizza tale carattere.
        \item Per ciascun nodo, il figlio sinistro rappresenta il bit 0 e il figlio destro rappresenta il bit 1.
    \end{itemize}
\end{itemize}

\noindent
Data una stringa $X$, vogliamo cercare un codice prefisso (e quindi un encoding tree) per i caratteri di $X$ tale che la codifica binaria di $X$ sia la più corta possibile.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.72\textwidth]{immagini/Greedy/ex_huffman.png}
    \caption{Esempio con $X = \text{"abracadabra"}$. L'encoding tree $T_1$ codifica $X$ in 29 bit, mentre l'encoding tree $T_2$ codifica $X$ in 24 bit, per cui la codifica di Huffman restituisce $T_2$ come soluzione ottimale.}
    \label{ex_huffman}
\end{figure}


\clearpage
\subsection*{Soluzione Greedy: l'algoritmo di Huffman}
L'algoritmo di codifica di Huffman inizia con ciascuno dei $d$ caratteri distinti della stringa $X$ da codificare ponendoli come nodo radice di un albero binario a nodo singolo. L'algoritmo procede in una serie di passaggi. In ogni passaggio, l'algoritmo prende i due alberi binari con le frequenze più piccole e li unisce in un unico albero binario. Ripete questo processo fino a quando non rimane un solo albero. 

Ogni iterazione del ciclo while nell'algoritmo di Huffman può essere implementata in tempo $O(\log d)$ utilizzando una \emph{coda di priorità rappresentata con un heap}. Inoltre, ogni iterazione estrae due nodi da $Q$ e ne aggiunge uno, un processo che verrà ripetuto $d - 1$ volte prima che rimanga esattamente un nodo in $Q$. Pertanto, questo algoritmo funziona in tempo $O(n + d \log d)$. 

Sebbene una giustificazione completa della correttezza di questo algoritmo sia al di fuori del nostro ambito qui, notiamo che la sua intuizione deriva da una semplice idea: qualsiasi codice ottimale può essere convertito in un codice ottimale in cui le code-word per i due caratteri a frequenza più bassa, $a$ e $b$, differiscono solo nel loro ultimo bit. Reiterando il ragionamento per una sequenza in cui $a$ e $b$ sono sostituiti da un unico carattere $c$, si ottiene quanto segue:

\vspace{1\baselineskip}
\hrule

\begin{verbatim}
Input:  String X of length n with d distinct characters
Output: Encoding tree for X

1.  Huffman(X):
2.      Compute the frequency f(c) of each character c of X
3.      Initialize a priority queue Q
4.      for each character c in X do
5.          Create a single-node binary tree T storing c
6.          Insert T into Q with key f(c)
7.      while len(Q) > 1 do
8.          (f_1, T_1) = Q.remove_min()
9.          (f_2, T_2) = Q.remove_min()
10.         Create a new binary tree T with left subtree T_1 and 
            right subtree T_2
11.         Insert T into Q with key f_1 + f_2
12.     (f, T) = Q.remove_min()
13.     return tree T
\end{verbatim}

\hrule
\vspace{1\baselineskip}


\clearpage
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/Greedy/ex1_huffman.png}
    \caption{Esempio di esecuzione dell'algoritmo di Huffman per la stringa $X = \text{"abracadabra"}$. Ipotizzando che ciascun carattere sia rappresentato con 8 bit, la codifica standard richiederebbe 11x8 = 88 bit. La codifica di Huffman riduce questo numero a 23 bit, risparmiando così il 73.86\% di spazio.}
    \label{ex1_huffman}
\end{figure}

\vspace{3\baselineskip}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/Greedy/ex2_huffman.png}
    \caption{Esempio di esecuzione dell'algoritmo di Huffman per la stringa $X = \text{"a fast runner need never be afraid of the dark"}$. Ipotizzando che ciascun carattere (compreso lo spazio vuoto) sia rappresentato con 8 bit, la codifica standard richiederebbe 46x8 = 368 bit. La codifica di Huffman riduce questo numero a 165 bit, risparmiando così il 55.16\% di spazio.}
    \label{ex2_huffman}
\end{figure}