\chapter{Dynamic Programming}
\label{cap:DynamicProgramming}

La \emph{programmazione dinamica} è una tecnica di progettazione di algoritmi per la risoluzione di problemi di ottimizzazione (così come l'approccio Greedy discusso nel capitolo precedente). 

Questa tecnica è simile alla tecnica \emph{divide-et-impera}, e proprio per questo motivo può essere applicata a una vasta gamma di problemi differenti. La programmazione dinamica può spesso essere utilizzata per trasformare problemi che sembrano richiedere un tempo esponenziale in algoritmi che li risolvono in tempo polinomiale. Inoltre, gli algoritmi che risultano dall'applicazione della tecnica di programmazione dinamica sono solitamente piuttosto semplici a livello concettuale.


\section{Modello generale}
Quando si progetta un algoritmo di programmazione dinamica è importante seguire in ordine i seguenti passi:

\begin{enumerate}
    \item Definire i \textbf{sotto-problemi}.
    \item Definire come la soluzione ottimale può essere ottenuta dalle soluzioni ottimali dei sotto-problemi ... e, ricorsivamente, come la soluzione ottimale di un sotto-problema può essere ottenuta dalla soluzione ottimale dei suoi sotto-problemi.
    \item Descrivere la soluzione ottimale attraverso un'\textbf{equazione caratteristica}.
    \begin{itemize}[nosep]
        \item Questa relazione lega la soluzione globale con le soluzioni dei sotto-problemi, e definisce inoltre i \emph{casi base} (\textbf{boundary conditions}), ovvero quei sotto-problemi le cui soluzioni sono banali e immediate, fondamentali per risolvere i sotto-problemi più grandi.
    \end{itemize}
\end{enumerate}

\noindent
È fondamentale scegliere i sotto-problemi in modo "intelligente", così come vedremo nei prossimi esempi, in modo da facilitare la definizione dell'equazione caratteristica e la risoluzione dei sotto-problemi stessi.


\clearpage
\noindent
La programmazione dinamica è per alcuni aspetti simile alla tecnica \emph{divide-et-impera}. Sebbene entrambe le tecniche suddividano il problema originale in porzioni più piccole, la differenza sostanziale risiede nella relazione tra questi sotto-problemi.
Nel paradigma \emph{divide-et-impera}, i sotto-problemi sono \textbf{indipendenti} (o disgiunti): la soluzione di un ramo non influenza né è necessaria per la risoluzione degli altri. 
Al contrario, la \emph{programmazione dinamica} è applicabile quando i sotto-problemi sono \textbf{sovrapposti} (\emph{overlapping subproblems}), ovvero quando condividono a loro volta dei sotto-problemi comuni.

Mentre un approccio \emph{divide-et-impera} puro ricalcolerebbe più volte la soluzione per lo stesso sotto-problema condiviso (portando spesso a una complessità esponenziale), la programmazione dinamica risolve ogni sotto-problema una sola volta e ne memorizza il risultato (\emph{memoization} o tabulazione) per riutilizzarlo in futuro, garantendo così l'efficienza polinomiale.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{immagini/Dynamic/pd_vs_dei.png}
    \caption{Esempio di risoluzione del problema della \emph{sequenza di Fibonacci} con divide-et-impera. Questa soluzione non è ottimale dal punto di vista computazionale, in quanto uno stesso sotto-problema viene risolto più volte (ad esempio, in figura f(6) viene calcolato 4 volte).}
    \label{fig:pd_vs_dei}
\end{figure}

\noindent
Una possibile soluzione che sfrutta la programmazione dinamica:
\begin{lstlisting}
def fibonacci(n):
    # Gestione caso base immediato
    if n <= 1:
        return n
    # Creazione della tabella (array) per memorizzare i risultati
    # Inizializzata a 0, dimensione n+1 per ospitarel'indice n
    table = [0] * (n + 1)
    # Impostazione dei casi base noti
    table[0] = 0
    table[1] = 1
    # Riempimento della tabella dal basso verso l'alto
    for i in range(2, n + 1):
        table[i] = table[i-1] + table[i-2]
    return table[n]
\end{lstlisting}


\clearpage
\section{Longest Common Subsequence (LCS)}
Una \emph{sottosequenza} di una stringa $x_0x_1x_2 \ldots x_{n-1}$ è una stringa $x_{i_0}x_{i_1} \ldots x_{i_k}$, dove $i_j < i_{j+1}$. In altre parole, una sottosequenza si ottiene eliminando alcuni caratteri dalla stringa originale senza cambiare l'ordine dei caratteri rimanenti.

Da notare che è diverso dal concetto di \emph{sottostringa}: una sottostringa è una sequenza contigua di caratteri all'interno della stringa originale, mentre una sottosequenza può essere formata da caratteri non contigui. Per cui, una sottostringa è sempre una sottosequenza, ma non viceversa.

\paragraph{Esempio}: Data la stringa "AGGTAB", alcune delle sue sottosequenze sono "GTA", "ATAB", "GAB", mentre alcune delle sue sottostringhe sono "AGG", "GGTA", "TAB".

\vspace{1\baselineskip}
\noindent
Uno dei problemi classici che può essere risolto in modo efficiente tramite la programmazione dinamica è il problema della \emph{Longest Common Subsequence} (LCS), ovvero la ricerca della sottosequenza comune più lunga tra due sequenze date.

\begin{itemize}
    \item Date due stringhe $X = x_0 x_1 \ldots x_{n-1}$ e $Y = y_0 y_1 \ldots y_{m-1}$ su di un alfabeto $\Sigma$, trovare la stringa più lunga che è una sottosequenza di entrambe le stringhe.
\end{itemize}

\vspace{2\baselineskip}
\subsection*{Soluzione Brute-Force}
Tutte le possibili sottosequenze di una stringa $X$ di lunghezza $n$ sono $2^n$. Quindi, un approccio brute-force per risolvere il problema LCS sarebbe generare tutte le sottosequenze di $X$, e per ognuna di esse verificare se è anche una sottosequenza di $Y$, tenendo traccia della più lunga trovata. Questo approccio ha una complessità temporale esponenziale di $O(2^n \cdot m)$, dove $m$ è la lunghezza della stringa $Y$.

\clearpage
\subsection*{Soluzione Dynamic Programming}
Sia $L_{n,m}$ la lunghezza della LCS tra le stringe $X = x_0 x_1 \ldots x_{n-1}$ e $Y = y_0 y_1 \ldots y_{m-1}$. $L_{n,m}$ rappresenta quindi la soluzione ottimale del problema.

\subsubsection*{Scomposizione in sotto-problemi}
Sia $L_{j,k}$ la lunghezza della LCS tra i prefissi $x_0 x_1 \ldots x_{j-1}$ e $y_0 y_1 \ldots y_{k-1}$, con $0 \leq j \leq n$ e $0 \leq k \leq m$. $L_{j,k}$ rappresenta quindi la soluzione ottimale al sotto-problema che considera solo i primi $j$ caratteri di $X$ e i primi $k$ caratteri di $Y$.

Osserviamo l'ultimo carattere di entrambe le stringhe considerate nello specifico sotto-problema, e da qui ricaviamo le due \textbf{equazioni caratteristiche} che ci permettono di esprimere $L_{j,k}$ in funzione dei sotto-problemi più piccoli:
\begin{itemize}[nosep]
    \item Se $x_{j-1} = y_{k-1}$, allora questo carattere fa parte della LCS, e possiamo scrivere:
    $$L_{j,k} = 1 + L_{j-1,k-1}$$
    \item Se $x_{j-1} \neq y_{k-1}$, allora l'ultimo carattere di almeno una delle due stringhe non fa parte della LCS, e possiamo scrivere:
    $$L_{j,k} = \max(L_{j-1,k}, L_{j,k-1})$$
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/Dynamic/lcs_equazioni.png}
    \caption{Rappresentazione grafica delle equazioni caratteristiche per il calcolo di $L_{j,k}$.}
    \label{fig:lcs_equazioni}
\end{figure}

\subsubsection*{Boundary Conditions}
Il caso base si verifica quando una delle due stringhe è vuota, ovvero quando $j = 0$ o $k = 0$. In questi casi, la LCS è anch'essa vuota, quindi:
$$L_{0,k} = 0 \quad \text{per } 0 \leq k \leq m$$
$$L_{j,0} = 0 \quad \text{per } 0 \leq j \leq n$$

\noindent
Notiamo che la soluzione $L_{j,k}$ appare nella computazione di:
$$ L_{j+1,k} \quad L_{j,k+1} \quad L_{j+1,k+1} $$
per cui i sotto-problemi si sovrappongono, rendendo la programmazione dinamica una tecnica adatta per risolvere questo problema. Quindi, anziché utilizzare un approccio ricorsivo che ricalcola più volte gli stessi sotto-problemi (divide-et-impera), possiamo utilizzare una tabella bidimensionale per memorizzare i risultati dei sotto-problemi già calcolati. 

\subsubsection*{Calcolo della tabella}
La tabella avrà dimensioni $(n+1) \times (m+1)$, dove l'elemento nella cella $(j,k)$ conterrà il valore di $L_{j,k}$, ovvero la LCS tra i primi $j$ caratteri di $X$ e i primi $k$ caratteri di $Y$. Inizializziamo la prima riga e la prima colonna della tabella con i valori dei casi base, e poi riempiamo la tabella utilizzando le equazioni caratteristiche definite sopra.

Anche l'ordine in cui viene riempita la tabella è importante: in questo caso (e in molti altri, ma dipende dal problema specifico) possiamo procedere per righe, in quanto la computazione di $L_{j,k}$ dipende solo dai valori presenti nelle equazioni caratteristiche, ovvero:
\begin{itemize}[nosep]
    \item $L_{j-1,k-1}$ (riga e colonna precedenti).
    \item $L_{j-1,k}$ (riga precedente e stessa colonna).
    \item $L_{j,k-1}$ (stessa riga e colonna precedente).
\end{itemize}

\vspace{1\baselineskip}
\begin{lstlisting}
def LCS(X, Y):
    """ Returns table such that L[j][k] is length of LCS for X[0:j] and Y[0:k] """
    n, m = len(X), len(Y)   
    L = [[0] * (m + 1) for k in range(n + 1)]   # (n+1) x (m+1) table
    for j in range(1, n + 1):           # j from 1 to n
        for k in range(1, m + 1):       # k from 1 to m
            if X[j-1] == Y[k-1]:        # match
                L[j][k] = 1 + L[j-1][k-1]
            else:                       # no match
                L[j][k] = max(L[j-1][k], L[j][k-1])
    return L
\end{lstlisting}
\vspace{1\baselineskip}

\noindent
L'algoritmo che si occupa di calcolare la tabella $L$ impiega due cicli annidati, iterando rispettivamente su $j$ e $k$. All'interno del ciclo più interno, viene eseguita una semplice operazione di confronto e un'assegnazione, entrambe con complessità $O(1)$, quindi la complessità totale dell'algoritmo è $O(n \cdot m)$, dove $n$ e $m$ sono le lunghezze delle stringhe $X$ e $Y$ rispettivamente. 


\subsubsection*{Estrazione della soluzione}
Come già detto, la tabella $L$ contiene le lunghezze delle LCS per tutti i sotto-problemi, ma non restituisce direttamente la LCS stessa. Tuttavia, è possibile ricostruire la LCS a partire dalla tabella $L$. La soluzione può essere ricavata, a partire dalla cella $L[n][m]$ nel modo seguente:

\clearpage
\begin{itemize}[nosep]
    \item Considerando una generica cella $L[j][k]$:
    \begin{itemize}[nosep]
        \item Se $c = x_{j-1} = y_{k-1}$, significa che questo carattere comune $c$ ha contribuito alla lunghezza della sottosequenza basandosi sul valore precedente $L_{j-1,k-1}$. Possiamo quindi registrare $x_{j-1}$ come parte della soluzione e proseguire l'analisi dalla cella $L_{j-1,k-1}$.
        \item Se $x_{j-1} \neq y_{k-1}$, allora la lunghezza della LCS dipende dal massimo tra $L[j][k-1]$ e $L[j-1][k]$. In questo caso, dobbiamo spostarci nella direzione del massimo valore per continuare la ricerca della LCS.
        \item Continuiamo questo processo fino a raggiungere una cella $L[j][k] = 0$.
    \end{itemize}
\end{itemize}


\vspace{1\baselineskip}
\begin{lstlisting}
def LCS_solution(X, Y, L):
    """ Returns the LCS of X and Y, given LCS table L """
    solution = []
    j, k = len(X), len(Y)
    while L[j][k] > 0:          # common characters remain
        if X[j-1] == Y[k-1]:          # match
            solution.append(X[j-1])   # add to solution
            j -= 1
            k -= 1
        elif L[j-1][k] >= L[j][k-1]:  # no match
            j -= 1
        else:
            k -= 1
    return ''.join(reversed(solution))  # return left-to-right LCS
\end{lstlisting}
\vspace{1\baselineskip}

\noindent
L'algoritmo per ricostruire la LCS ha una complessità temporale di $O(n + m)$, poiché in ogni iterazione del ciclo while si decrementa almeno uno tra $j$ e $k$, e il ciclo termina quando uno dei due raggiunge zero.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/Dynamic/lcs_table.png}
    \caption{Illustrazione dell'algoritmo per la costruzione di una longest common subsequence a partire dall'array L. Un passo diagonale da $L_{j,k}$ a $L_{j-1,k-1}$ sul percorso evidenziato rappresenta l'uso di un carattere comune, ovvero il carattere $c = x_{j-1} = y_{k-1}$.}
    \label{fig:lcs_table}
\end{figure}


\clearpage
\section{Edit Distance: Levenshtein Distance}
Il problema della \emph{Edit Distance} consiste nel trovare il numero minimo di operazioni necessarie per trasformare una stringa in un'altra stringa.
\begin{itemize}
    \item Date due stringhe $A = a_0 a_1 \ldots a_{n-1}$ e $B = b_0 b_1 \ldots b_{m-1}$ su di un alfabeto $\Sigma$, trovare il numero minimo di operazioni necessarie per trasformare $A$ in $B$. Le operazioni consentite sono: Inserimento(Insert), Cancellazione(Delete), Sostituzione(Replace).
\end{itemize} 

\noindent
L'\emph{Edit Transcript} di due stringhe $A$ e $B$ è una stringa $T$ sull'alfabeto $\{I, D, R, M\}$ (Insert, Delete, Replace, Match) che descrive una sequenza di operazioni per trasformare $A$ in $B$. 
Se in $T$ sostituiamo tutte le $I$ con $D$, e tutte le $D$ con $I$, otteniamo l'edit transcript $T'$ che trasforma $B$ in $A$.

\subsection*{Soluzione Dynamic Programming}
Sia $D(n,m)$ la distanza di edit tra le stringhe $A = a_0 a_1 \ldots a_{n-1}$ e $B = b_0 b_1 \ldots b_{m-1}$. $D_{n,m}$ rappresenta quindi la soluzione ottimale del problema.

\subsubsection*{Scomposizione in sotto-problemi}
Sia $D(i,j)$ la distanza di edit tra i prefissi $a_0 a_1 \ldots a_{i-1}$ e $b_0 b_1 \ldots b_{j-1}$, con $0 \leq i \leq n$ e $0 \leq j \leq m$. $D(i,j)$ rappresenta quindi la soluzione ottimale al sotto-problema che considera solo i primi $i$ caratteri di $A$ e i primi $j$ caratteri di $B$.

Anche in questo caso, osserviamo l'ultimo carattere di entrambe le stringhe, e da qui ricaviamo le \textbf{equazioni caratteristiche} che ci permettono di esprimere $D(i,j)$ in funzione dei sotto-problemi più piccoli:
\begin{itemize}[nosep]
    \item Se $a_{i-1} = b_{j-1}$ (\textbf{match}), allora non è necessaria alcuna operazione, e possiamo scrivere:
    $$ D(i,j) = D(i-1,j-1) = D(i-1,j-1) + t(a_{i-1},b_{j-1}) $$
    \item Se c'è una \textbf{replace} da $a_{i-1}$ a $b_{j-1}$:
    $$ D(i,j) = D(i-1,j-1) + 1 = D(i-1,j-1) + t(a_{i-1},b_{j-1}) $$
    \item se c'è una \textbf{delete} di $a_{i-1}$:
    $$ D(i,j) = D(i-1,j) + 1 $$
    \item se c'è una \textbf{insert} di $b_{j-1}$ dopo il carattere $a_{i-1}$:
    $$ D(i,j) = D(i,j-1) + 1 $$
\end{itemize}

\noindent
Dove la funzione di costo $t(a,b)$ è definita come:
\[t(a,b) = \begin{cases}
0 & \text{se } a = b \\
1 & \text{se } a \neq b
\end{cases}
\]

\clearpage
Si è in grado quindi di riassumere le equazioni caratteristiche in un'unica espressione:
\[ D(i,j) = \min \begin{cases}
D(i-1,j-1) + t(a_{i-1},b_{j-1}) \\
D(i-1,j) + 1 \\
D(i,j-1) + 1 
\end{cases} \]

\subsubsection*{Boundary Conditions}
È immediato riconoscere i casi base:
\begin{itemize}[nosep]
    \item $D(0,0) = 0$, in quanto la distanza di edit tra due stringhe vuote è zero.
    \item Se una delle due stringhe è vuota, la distanza di edit è pari alla lunghezza dell'altra stringa, in quanto sono necessarie tante operazioni di inserimento o cancellazione quanti sono i caratteri della stringa non vuota. Quindi:
    $$ D(0,j) = j \quad \text{per } 0 \leq j \leq m $$
    $$ D(i,0) = i \quad \text{per } 0 \leq i \leq n $$
\end{itemize}

\subsubsection*{Calcolo della tabella}
La tabella avrà dimensioni $(n+1) \times (m+1)$, dove l'elemento nella cella $(i,j)$ conterrà il valore di $D(i,j)$, ovvero la distanza di edit tra i primi $i$ caratteri di $A$ e i primi $j$ caratteri di $B$. Inizializziamo la prima riga e la prima colonna della tabella con i valori dei casi base, e poi riempiamo la tabella utilizzando l'equazione caratteristica definita sopra.

Notiamo inoltre che la soluzione $D(i,j)$ appare nella computazione di:

\noindent
$ D(i+1,j) \quad D(i,j+1) \quad D(i+1,j+1) $ per cui riempiamo la tabella riga per riga (come LCS).

\vspace{1\baselineskip}
\hrule
\begin{alltt}
EditDistance(\(A, B\))
1.  \textbf{int} \(n = \text{len(A)}\)
2.  \textbf{int} \(m = \text{len(B)}\)
3.  \textbf{int} \(D[n+1][m+1]\)
4.  \textbf{for} \(i\) \textbf{from} 0 \textbf{to} \(n\) \textbf{do}
5.      \(D[i][0] = i\)
6.  \textbf{for} \(j\) \textbf{from} 0 \textbf{to} \(m\) \textbf{do}
7.      \(D[0][j] = j\)
8.  \textbf{for} \(i\) \textbf{from} 1 \textbf{to} \(n\) \textbf{do}
9.      \textbf{for} \(j\) \textbf{from} 1 \textbf{to} \(m\) \textbf{do}
10.          \(cost = 0\) \textbf{if} \(A[i-1] == B[j-1]\) \textbf{else} 1
11.          \(D[i][j] = \min(D[i-1][j-1] + cost,\)
12.                        \(D[i-1][j] + 1,\)
13.                        \(D[i][j-1] + 1)\)
14. \textbf{return} \(D[n][m]\)
\end{alltt}
\hrule 

\clearpage
\noindent
L'algoritmo che si occupa di calcolare la tabella $D$ impiega due cicli annidati (oltre ai cicli di inizializzazione), iterando rispettivamente su $i$ e $j$. All'interno del ciclo più interno, viene eseguita una semplice operazione di confronto, una selezione condizionale e un'assegnazione, tutte con complessità $O(1)$, quindi la complessità totale dell'algoritmo è $O(n \cdot m)$, dove $n$ e $m$ sono le lunghezze delle stringhe $A$ e $B$ rispettivamente.

\subsubsection*{Estrazione della soluzione}
Anche in questo caso, la tabella $D$ contiene le distanze di edit per tutti i sotto-problemi, ma non restituisce direttamente la sequenza di operazioni necessarie per trasformare $A$ in $B$. Tuttavia, è possibile ricostruire questa sequenza a partire dalla tabella $D$ in tempo $O(n + m)$. 
\begin{itemize}
    \item Vogliamo trovare il percorso ottimale, ovvero quello che porta dalla cella $D(n,m)$ alla cella $D(0,0)$.
    \item Per fare ciò, per ciascuna cella $D(i,j)$ memorizziamo come il suo valore è stato calcolato.
    \begin{itemize}[nosep]
        \item Per esempio, se $D(i,j) = D(i,j-1) + 1$, allora sappiamo il valore della cella $D(i,j)$ è stato ottenuto dalla cella $D(i,j-1)$ tramite inserimento del carattere $b_{j-1}$.
    \end{itemize}
\end{itemize}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\textwidth]{immagini/Dynamic/edit_distance.png}
    \caption{Esempio di tabella per il calcolo della Edit Distance dalla stringa "vintner" alla stringa "winters". Le frecce indicano il percorso ottimale per trasformare la stringa $A$ (vintner) nella stringa $B$ (winters).}
    \label{fig:edit_distance}
\end{figure}

La freccia orizzontale ($\leftarrow$, da $D(i,j)$ a $D(i,j-1)$) rappresenta un'operazione di \textbf{inserimento} (Insert) del carattere $b_{j-1}$ in $A$.

La freccia verticale ($\uparrow$, da $D(i,j)$ a $D(i-1,j)$) rappresenta un'operazione di \textbf{cancellazione} (Delete) del carattere $a_{i-1}$ da $A$. 

La freccia diagonale ($\nwarrow$, da $D(i,j)$ a $D(i-1,j-1)$) rappresenta un'operazione di \textbf{match} (Match) se $a_{i-1} = b_{j-1}$ o di \textbf{sostituzione} (Replace) se $a_{i-1} \neq b_{j-1}$.

\clearpage
\noindent
In riferimento all'esempio in Figura \ref{fig:edit_distance}, per capire \textit{quali} operazioni abbiamo fatto, dobbiamo partire dalla fine (la soluzione ottimale) e ripercorrere i passi all'indietro fino all'inizio.

\paragraph{Gli Indici}
Per prima cosa, fissiamo la notazione per gli indici:
\begin{itemize}
    \item \textbf{La Cella corrente} $D(i, j)$: È la casella della tabella in cui ci troviamo adesso.
    \item \textbf{$i$ e $j$}: Sono le \textbf{lunghezze} dei prefissi. Se ad esempio siamo in $D(3, 2)$, stiamo guardando una sottostringa di $A$ lunga 3 e una di $B$ lunga 2, per cui gli indici reali degli ultimi caratteri sono $2$ e $1$ rispettivamente.
    \item \textbf{$i-1$ e $j-1$}: Sono gli \textbf{indici reali} dei caratteri nelle stringhe (poiché gli array partono da 0).
    \begin{itemize}
        \item Il carattere ``sotto esame'' della stringa $A$ è $a_{i-1}$.
        \item Il carattere ``sotto esame'' della stringa $B$ è $b_{j-1}$.
    \end{itemize}
\end{itemize}

\paragraph{Il Procedimento: Camminare all'Indietro}
Partiamo dalla cella in basso a destra $D(n, m)$ e ci chiediamo: \textit{``Da quale vicino sono arrivato qui pagando il costo minore?''}

\begin{enumerate}
    \item \textbf{Controllo Diagonale (Match o Replace):}
    Guardiamo la cella $D(i-1, j-1)$.
    \begin{itemize}
        \item Se $a_{i-1} == b_{j-1}$ (i caratteri sono uguali) e il costo non è cambiato, allora c'è stato un \textbf{Match}. Decrementiamo sia $i$ che $j$.
        \item Se i caratteri sono diversi e $D(i, j) = D(i-1, j-1) + 1$, allora c'è stata una \textbf{Sostituzione} (Replace). Decrementiamo sia $i$ che $j$.
    \end{itemize}

    \item \textbf{Controllo Sinistra (Insert):}
    Guardiamo la cella $D(i, j-1)$.
    Se $D(i, j) = D(i, j-1) + 1$, significa che siamo arrivati qui aggiungendo un carattere. L'operazione è stata una \textbf{Inserimento} del carattere $b_{j-1}$ dopo il carattere $a_{i-1}$.
    \begin{itemize}
        \item Ci spostiamo a sinistra: $j$ diventa $j-1$, ma $i$ resta uguale (perché non abbiamo consumato nulla da $A$).
    \end{itemize}

    \item \textbf{Controllo Sopra (Delete):}
    Guardiamo la cella $D(i-1, j)$.
    Se $D(i, j) = D(i-1, j) + 1$, significa che siamo arrivati qui rimuovendo un carattere superfluo. L'operazione è stata una \textbf{Cancellazione} del carattere $a_{i-1}$.
    \begin{itemize}
        \item Ci spostiamo in alto: $i$ diventa $i-1$, ma $j$ resta uguale (perché non abbiamo avanzato nella costruzione di $B$).
    \end{itemize}
\end{enumerate}

\noindent
Ripetiamo questi controlli finché non arriviamo alla cella $D(0, 0)$. La sequenza di operazioni trovata (letta al contrario) è la soluzione.






















\clearpage
\section{Sequence Alignment}
Un altro modo per misurare la somiglianza tra due stringhe è attraverso il \emph{sequence alignment}. 

\begin{itemize}
    \item Siano date due stringhe $X = x_1 x_2 \ldots x_m$ e $Y = y_1 y_2 \ldots y_{n}$.
    \item Siano $S = \{1,2,\ldots,m\}$ e $T = \{1,2,\ldots,n\}$ gli insiemi degli indici rispettivamente di $X$ e $Y$.
    \item Un \textbf{matching} tra $X$ e $Y$ consiste in un insieme $M$ di coppie $(x,y)$ in cui:
    \begin{itemize}[nosep]
        \item $x \in S$ e $y \in T$.
        \item Ogni indice di $S$ compare al più una volta in $M$.
        \item Ogni indice di $T$ compare al più una volta in $M$.
    \end{itemize}
    \item Un \textbf{alignment} di $X$ e $Y$ è un \emph{matching} $M$ di queste due stringhe, senza "crossing pairs" (coppie incrociate). 
    \begin{itemize}
        \item In altre parole, se $(i,j), (i',j') \in M$ e $i < i'$, allora deve essere $j < j'$.
    \end{itemize}
    \item Un \emph{alignment} ci dice quali coppie di posizioni allineate tra loro.
\end{itemize}


\noindent
\begin{minipage}[c]{0.15\textwidth}
    \hrule
    \vspace{0.5\baselineskip}
    \raggedright
    \texttt{stop-} \\
    \texttt{-tops}
    \vspace{0.5\baselineskip}
    \hrule
\end{minipage}
\hfill % Spazio flessibile tra le due colonne
\begin{minipage}[c]{0.80\textwidth}
    l'alignment corrispondente è: $\{(2,1), (3,2), (4,3)\}$.
\end{minipage}

\vspace{2\baselineskip}
\noindent
\begin{itemize}
    \item Sia $M$ un dato alignment tra $X$ e $Y$.
    \begin{itemize}
        \item Per ogni posizione di $X$ e $Y$ che non è matchata in $M$ (un gap) si ha un \textbf{gap penalty} di $\delta > 0$.
        \item Per ogni coppia $(i,j) \in M$ tale che $x_i = y_i$, non abbiamo alcun penalty.
        \item Per ogni coppia $(i,j) \in M$ tale che $x_i = p, y_j = q$ e $p \neq q$, abbiamo un \textbf{mismatch penalty} di $\alpha_{pq} > 0$.
    \end{itemize}
    \item L'\textbf{alignment cost} di $M$ è la somma dei suoi \emph{gap} e \emph{mismatch} penalties. I valori di $\delta$ e $\alpha_{pq}$ sono specificati come parte del problema e dipendono dalla specifica applicazione.
\end{itemize}

\noindent
\begin{minipage}[c]{0.16\textwidth}
    \hrule
    \vspace{0.5\baselineskip}
    \raggedright
    \texttt{o-currance} \\
    \texttt{occurrence}
    \vspace{0.5\baselineskip}
    \hrule
\end{minipage}
\hfill % Spazio flessibile tra le due colonne
\begin{minipage}[c]{0.80\textwidth}
    Alignment costs $\delta + \alpha_{ae}$
\end{minipage}

\vspace{1\baselineskip}
Cerchiamo un alignment tra $X$ e $Y$ che minimizzi il costo totale con qualsiasi $\delta$ e $\alpha_{pq}$.
\vspace{1\baselineskip}

\noindent
\begin{minipage}[c]{0.16\textwidth}
    \hrule
    \vspace{0.5\baselineskip}
    \raggedright
    \texttt{o-curr-ance} \\
    \texttt{occurre-nce}
    \vspace{0.5\baselineskip}
    \hrule
\end{minipage}
\hfill % Spazio flessibile tra le due colonne
\begin{minipage}[c]{0.80\textwidth}
    Alignment costs $3 \delta$
\end{minipage}

\clearpage
\subsection*{Soluzione Dynamic Programming}
Nell'alignment ottimale $M$, la coppia $(m,n)$ può essere presente o meno in $M$. Inoltre, gli ultimi caratteri delle due stringhe possono essere matchati tra loro oppure no. 

\begin{itemize}
    \item Se $(m,n) \notin M$, allora o la posizione $m$ di $X$ o la posizione $n$ di $Y$ non è matchata in $M$.
    \begin{itemize}[nosep]
        \item Altrimenti, ci sarebbero $(m,j)$ e $(i,n)$ in $M$ con $j < n$ e $i < m$.
        \item Impossibile, poiché sarebbero coppie incrociate.
    \end{itemize}
    \item Quindi, in un alignment ottimale $M$, almeno una delle seguenti condizioni è vera:
    \begin{itemize}[nosep]
        \item $(m,n) \in M$ e paghiamo $\alpha_{pq}$ dove $p = x_m$ e $q = y_n$ (assumendo che $\alpha_{pp} = 0$).
        \item La posizione $m$ di $X$ non è matchata e paghiamo $\delta$.
        \item La posizione $n$ di $Y$ non è matchata e paghiamo $\delta$.
        \end{itemize}
\end{itemize}

\noindent
Denotiamo con $\text{OPT}(i,j)$ il costo minimo di alignment tra $x_1 x_2 \ldots x_i$ e $y_1 y_2 \ldots y_j$. 

\begin{itemize}
    \item Se scegliamo di inserire $(i,j)$ nell'alignment, allora dobbiamo pagare $\alpha_{pq}$, dove $p = x_i$, e $q = y_j$, più il costo del miglior alignment tra $x_1 x_2 \ldots x_{i-1}$ e $y_1 y_2 \ldots y_{j-1}$.
    \[
        \text{OPT}(i,j) = \alpha_{pq} + \text{OPT}(i-1,j-1)
    \]
    \item Se scegliamo di non matchare $x_i$, allora dobbiamo pagare un gap cost di $\delta$ più il costo del miglior alignment tra $x_1 x_2 \ldots x_{i-1}$ e $y_1 y_2 \ldots y_j$.
    \[
        \text{OPT}(i,j) = \delta + \text{OPT}(i-1,j)
    \]
    \item Se scegliamo di non matchare $y_j$, allora dobbiamo pagare un gap cost di $\delta$ più il costo del miglior alignment tra $x_1 x_2 \ldots x_i$ e $y_1 y_2 \ldots y_{j-1}$.
    \[
        \text{OPT}(i,j) = \delta + \text{OPT}(i,j-1)
    \]
\end{itemize}

Tra tutte queste possibilità, scegliamo quella che minimizza il costo totale:
\[
    \text{OPT}(i,j) = \min \begin{cases}
    \alpha_{pq} + \text{OPT}(i-1,j-1) \\
    \delta + \text{OPT}(i-1,j) \\
    \delta + \text{OPT}(i,j-1)
    \end{cases}
\]

\noindent
Le \textbf{boundary conditions} sono date dal fatto che l'allineamento di una stringa vuota con una stringa di lunghezza $i$ richiede $i$ gap, ovvero $i$ volte il costo di gap $\delta$:
\[
    \text{OPT}(i,0) = \text{OPT}(0,i) = i \cdot \delta
\]


\clearpage
\noindent
A questo punto non ci resta che mettere tutto insieme:
\begin{itemize}
    \item Costruiamo una tabella contenente i valori di $\text{OPT}(i,j)$ per $0 \leq i \leq m$ e $0 \leq j \leq n$.
    \begin{itemize}[nosep]
        \item Il tempo impiegato per questa operazione è $O(m \cdot n)$, dato che la tabella ha $O(m \cdot n)$ celle, e ciascuna può essere calcolata in tempo $O(1)$.
    \end{itemize}
    \item Il valore di $\text{OPT}(m,n)$ rappresenta il costo minimo di alignment tra le due stringhe $X$ e $Y$, e quindi la soluzione al problema originale.
    \item Come già visto nei casi precedenti, possiamo tracciare il percorso attraverso la tabella per ricostruire l'alignment ottimale stesso.
    \begin{itemize}[nosep]
        \item Il tempo impiegato per questa operazione è $O(m + n)$, poiché in ogni passo si decrementa almeno uno tra $i$ e $j$, e il processo termina quando uno dei due raggiunge zero.
    \end{itemize}
\end{itemize}


\clearpage
\section{Coin Change per sistemi non canonici}
Il problema del \emph{Coin Change}, così come già visto nel Capitolo \ref{cap:Greedy}, consiste nel trovare il numero minimo di monete necessarie per eguagliare una certa somma, dato un insieme di tagli di monete disponibili. La soluzione greedy funziona correttamente solo per sistemi di monete canonici, ovvero quei sistemi in cui la scelta della moneta di taglio più grande possibile in ogni passo porta sempre alla soluzione ottimale. Tuttavia, esistono sistemi di monete non canonici in cui l'approccio greedy non garantisce la soluzione ottimale.

\subsection*{Soluzione Dynamic Programming: $1^o$ approccio}
Per risolvere il problema del Coin Change in sistemi non canonici, possiamo utilizzare un approccio di programmazione dinamica. 
\begin{itemize}
    \item Sia dato un insieme di $n$ tagli di monete $1 = c_1 < c_2 < \ldots < c_n$.
    \item Definiamo $C[r]$ come il numero minimo di monete necessarie per eguagliare il resto $r$.
\end{itemize}
La soluzione ottimale può essere espressa in termini di soluzioni ottimali a sotto-problemi più piccoli.
L'idea di base è che se in un dato step la soluzione ottimale utilizza una moneta di valore $c_i$, allora $C[r] = 1 + C[r - c_i]$. Quindi, possiamo esprimere $C[r]$ come:

$$
C[r] = 
\begin{cases} 
   \infty & \text{se } r < 0 \\
   0 & \text{se } r = 0 \\
   1 + \min\limits_{1 \le i \le n} \{ C[r - c_i] \} & \text{se } r \ge 1 
\end{cases}
$$

\vspace{1\baselineskip}
\noindent
Questo è un possibile approccio per scomporre il problema in sotto-problemi più piccoli. Sviluppando questo ragionamento, avremmo una soluzione che impiega un tempo $O(n \cdot r)$. 
A differenza degli esempi precedenti, in questo caso non c'è una tabella bidimensionale, ma un array monodimensionale $C$ di lunghezza $r + 1$, dove ogni elemento $C[j]$ rappresenta il numero minimo di monete necessarie per eguagliare il resto $j$. Questo approccio utilizza la programmazione dinamica per salvare i risultati intermedi, evitando di dover ricalcolare più volte la stessa soluzione. L'unica boundary condition è banalmente $C[0] = 0$, ovvero non è necesssaria alcuna moneta per eguagliare il resto zero; tutti gli altri valori $C[j]$ per $j \ge 1$ vengono derivati calcolando il minimo tra le opzioni disponibili.

\clearpage
\subsection*{Soluzione Dynamic Programming: $2^o$ approccio}
Riprendiamo le assunzioni fatte prima ma in maniera leggermente diversa:
\begin{itemize}
    \item Sia dato un insieme di $n$ tagli di monete $1 = c_1 < c_2 < \ldots < c_n$.
    \item Definiamo $M(i,r)$ come il numero minimo di monete $(c_1, c_2, ..., c_i)$, con $i \leq n$, necessarie per eguagliare il resto $r$.
\end{itemize}
L'idea di base è che in un dato step possiamo scegliere se includere o meno la moneta di taglio $c_i$ nella soluzione ottimale. Quindi, possiamo esprimere $M(i,r)$ come:

$$
M(i,r) = 
\begin{cases}
    M(i-1,r) & \text{se non inseriamo } c_i \text{ nella soluzione}\\
    M(i, r-c_i) & \text{se inseriamo } c_i \text{ nella soluzione}
\end{cases}
$$

\noindent
Le boundary conditions sono le seguenti:
\begin{itemize}
    \item $M(i,0) = 0$, in quanto per eguagliare il resto zero non è necessaria alcuna moneta.
    \item $M(1,r) = r$, se abbiamo solo monete di valore unitario $c_1 = 1$ l'unica soluzione possibile è utilizzare $r$ monete.
    \item $M(i,r) = 1$ se $c_i = r$, in quanto possiamo eguagliare il resto $r$ con una sola moneta di valore $c_i$.
    \item $M(i,r) = M(i-1,r)$ se $c_i > r$, in quanto non possiamo utilizzare la moneta di taglio $c_i$ se è maggiore del resto $r$.
\end{itemize}

\vspace{1\baselineskip}
\begin{lstlisting}
def dp_coin_change(amount, coins=[0, 1, 2, 5, 10, 20, 50]):
    # coins[0] (che è 0) non viene usato, serve solo a scalare gli indici
    nc = len(coins)
    # Crea una matrice di nc righe. Nota: m[0] sarà la riga inutilizzata
    m = [[0]*(amount+1) for _ in range(nc)] 
    # Caso base: gestiamo la prima moneta reale (coins[1] = 1)
    # Scriviamo sulla riga m[1]
    for r in range(amount+1):
        m[1][r] = r

    # Ciclo principale: partiamo dalla seconda moneta reale (indice 2)
    for i in range(2, nc):
        for r in range(1, amount+1):
            if coins[i] == r:
                m[i][r] = 1
            elif coins[i] > r:
                m[i][r] = m[i-1][r]
            else:
                m[i][r] = min(m[i-1][r], m[i][r-coins[i]] + 1)
                
    return m, m[-1][-1]     # ritorna la matrice e il risultato finale
\end{lstlisting}

\noindent
Una volta popolata la matrice $M$, la sequenza esatta di monete che costituisce la soluzione ottimale si ricava mediante una procedura di \textit{backtracking}. Si parte dalla cella finale $M[n][R]$ (dove $n$ è il numero di tipi di monete e $R$ l'importo totale) e si procede a ritroso fino a raggiungere un resto nullo ($r=0$).
Ad ogni passo, trovandosi nella cella $(i, r)$, si confronta il valore corrente con quello della riga immediatamente superiore, $M[i-1][r]$:
\begin{itemize}
    \item Se $M[i][r] = M[i-1][r]$, significa che la moneta di taglio $c_i$ non è stata utilizzata per ottenere l'ottimo locale (la soluzione migliore era già stata trovata con le monete precedenti). In questo caso, ci si sposta semplicemente alla riga superiore: $(i-1, r)$.
    \item Se invece $M[i][r] < M[i-1][r]$, implica che la moneta $c_i$ è stata utilizzata, facendo diminuire il numero di monete necessarie ($M[i][r] < M[i-1][r]$). Si aggiunge $c_i$ all'elenco delle monete scelte e si sottrae il suo valore dal resto residuo, spostandosi alla cella $(i, r - c_i)$. Si rimane sulla riga $i$ poiché, nel problema con ripetizioni, la stessa moneta potrebbe essere stata usata più volte.
\end{itemize}


\clearpage
\section{0-1 Knapsack}
Il problema dello \emph{0-1 Knapsack} si differenzia dal problema del \emph{Fractional Knapsack} (Capitolo \ref{cap:Greedy}) per il fatto che ogni oggetto può essere inserito nello zaino solo per intero e non in frazioni. In altre parole, ogni oggetto può essere inserito nello zaino solo per intero o non essere inserito affatto.

\begin{itemize}
    \item Dati $n$ oggetti, ciascuno con un volume $v_i$ e un costo/valore $c_i$, e un limite di volume $B$, trovare un sottoinsieme $S \subseteq \{1, \ldots, n\}$ di oggetti con volume totale al più $B$ tale che il costo totale sia massimizzato.
\end{itemize}

\noindent
Come già accennato in precedenza, il problema dello 0-1 Knapsack non può essere risolto in modo ottimale utilizzando un approccio greedy, poiché la scelta dell'oggetto con il miglior rapporto costo/volume non garantisce una soluzione ottimale globale, ma questa soluzione funzionava solo per il problema frazionario. Tuttavia, possiamo utilizzare un approccio di programmazione dinamica per risolvere questo problema in modo efficiente.

\subsection*{Soluzione Dynamic Programming}
Assumendo che i volumi $v_i$ siano interi positivi, definiamo i sotto-problemi come segue:
\begin{itemize}
    \item Consideriamo solamente i primi $i$ oggetti $x_1, x_2, \ldots, x_i$, ed un volume massimo $j$ dello zaino, con $0 \leq i \leq n$ e $0 \leq j \leq B$.
    \item Definiamo $V(i,j)$ come il valore massimo ottenibile utilizzando i primi $i$ oggetti con un volume massimo $j$ dello zaino.
\end{itemize}

\noindent
Il valore di $V(i,j)$ dipende dalla scelta di includere o meno l'oggetto $x_i$ nello zaino:

\begin{itemize}
    \item $V(i,j) = V(i-1,j)$, se scegliamo di non inserire l'oggetto $x_i$ nello zaino.
    \item $V(i,j) = c_i + V(i-1,j-v_i)$, se scegliamo di inserire l'oggetto $x_i$ nello zaino (a condizione che $v_i \leq j$).
\end{itemize}

\noindent
Dal momento che vogliamo massimizzare il valore totale, l'\textbf{equazione caratteristica} diventa:

$$
V(i,j) = 
\begin{cases}
    \max(V(i-1,j), \quad c_i + V(i-1,j-v_i)) & \text{se } v_i \leq j \\
    V(i-1,j) & \text{se } v_i > j 
\end{cases}
$$

\noindent
Le boundary conditions sono le seguenti:
\begin{itemize}[nosep]
    \item $V(i,0) = 0$ per ogni $1 \leq i \leq n$, poiché se il volume dello zaino è zero, non possiamo inserire alcun oggetto.
    \item $V(0,j) = 0$ per ogni $1 \leq j \leq B$, poiché se non abbiamo oggetti a disposizione, il valore massimo ottenibile è zero.
    \item $V(1,j) = v_1$ se $v_1 \leq j$, altrimenti $V(1,j) = 0$, poiché con un solo oggetto possiamo inserirlo nello zaino solo se il suo volume è minore o uguale al volume massimo dello zaino. 
\end{itemize}

\clearpage
\noindent
Un algoritmo iterativo può riempire la matrice calcolando ogni voce in tempo costante. Alla fine, il costo della soluzione ottimale è riportato nella cella $V(n,B)$. Il tempo necessario da un algoritmo per riempire la tabella è $O(nB)$. 

Per costruire una soluzione ottimale, possiamo anche costruire una matrice booleana $C$ della stessa dimensione $n \times (B + 1)$. Per ogni $1 \leq i \leq n$ e $0 \leq j \leq B$, $C(i,j) = \text{True}$ se e solo se esiste una soluzione ottimale che inserisce un sottoinsieme dei primi $i$ oggetti nel volume $j$ in modo che l'oggetto $i$ sia incluso nella soluzione. Utilizzando la matrice $C$ possiamo lavorare a ritroso per ricostruire gli elementi presenti in una soluzione ottimale. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{immagini/Dynamic/knapsack.png}
    \caption{Esempio con $n = 9$ oggetti e limite di volume $B = 15$. La tabella $V$ presenta le righe stampate in ordine decrescente, ma questo non è un problema ma riguarda solo una scelta di impaginazione. La soluzione ottimale ha valore $V(9,15) = 23$, ottenuta selezionando gli oggetti: 9, 7, 5, 4}
    \label{fig:knapsack}
\end{figure}

\clearpage
\vspace{1\baselineskip}
\hrule
\begin{alltt}
    Input:
        \(v\): array dei volumi (\(v[1] \dots v[n]\))
        \(c\): array dei costi (\(c[1] \dots c[n]\))
        \(B\): capacità massima
  Output:
        \(V\): Tabella delle soluzioni ottime
        \(C\): Tabella delle scelte (inclusione oggetto)
------------------------------------------------------------------------
ALGORITMO Knapsack01(\(v, c, B\))
 1. \(n = \text{len}(v)\)
 2. \textbf{for} \(j = 0\) \textbf{to} \(B\) \textbf{do}   // Inizializzazione riga 0 (nessun oggetto)
 3.     \(V[0, j] = 0\)
 4. \textbf{for} \(i = 0\) \textbf{to} \(n\) \textbf{do}   // Inizializzazione colonna 0 (capacità 0)
 5.     \(V[i, 0] = 0\)
    // Ciclo principale: \(i\) scorre gli oggetti, \(j\) le capacità
 6. \textbf{for} \(i = 1\) \textbf{to} \(n\) \textbf{do}
 7.     \textbf{for} \(j = 1\) \textbf{to} \(B\) \textbf{do}

            // 1. L'oggetto \(i\) ha volume maggiore della capacità attuale \(j\)?
            // Se sì, non entra fisicamente nello zaino.
 8.         \textbf{if} \(v[i] > j\) \textbf{then}
 9.             \(V[i, j] = V[i-1, j]\)
10.             \(C[i, j] = \text{False}\)

            // 2. L'oggetto entra, ma conviene prenderlo?
            // Se il valore SENZA l'oggetto (\(V[i-1, j]\)) è MAGGIORE
            // del valore CON l'oggetto, allora NON lo prendiamo.
11.         \textbf{else if} \(V[i-1, j] > c[i] + V[i-1, j - v[i]]\) \textbf{then}
12.             \(V[i, j] = V[i-1, j]\)
13.             \(C[i, j] = \text{False}\)

            // 3. L'oggetto entra e conviene prenderlo.
            // (Il valore CON l'oggetto è maggiore o uguale al precedente).
14.         \textbf{else}
15.             \(V[i, j] = c[i] + V[i-1, j - v[i]]\)
16.             \(C[i, j] = \text{True}\)

17. \textbf{return} \(V, C\)
\end{alltt}
\hrule 


\clearpage
\section{Matrix Chain-Product}
Supponiamo di avere una collezione di $n$ matrici $A_1, A_2, \ldots, A_n$ e vogliamo calcolare il loro prodotto $A$. La matrice $A_i$ ha dimensioni $d_i \times d_{i+1}$, per $i = 0, 1, 2, \ldots, n-1$.

Ricordiamo la definizione formale di moltiplicazione tra due matrici $B$ di dimensioni $d \times e$ e $C$ di dimensioni $e \times f$:
$$
A[i][j] = \sum_{k=0}^{e-1} B[i][k] \cdot C[k][j] \quad \text{richiede: } d \cdot e \cdot f \text{ moltiplicazioni scalari}
$$

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.8\textwidth]{immagini/Dynamic/matrix_multiplication.png}
    \caption{Esempio di moltiplicazione tra tre matrici $B$ e $C$.}
    \label{fig:matrix_multiplication}
\end{figure}

\noindent
La moltiplicazione di matrici è un'operazione associativa, il che significa che $B \cdot (C \cdot D) = (B \cdot C) \cdot D$. Pertanto, possiamo inserire parentesi nell'espressione in qualsiasi modo desideriamo e otterremo lo stesso risultato. Tuttavia, modificando l'ordine delle parentesi possiamo influenzare il numero di moltiplicazioni scalari necessarie per calcolare il prodotto finale, come illustrato nell'esempio seguente.

\paragraph{Esempio:} Sia $B$ una matrice di dimensioni $2 \times 10$, $C$ una matrice di dimensioni $10 \times 50$ e $D$ una matrice di dimensioni $50 \times 20$. Calcolare il prodotto $B \cdot (C \cdot D)$ richiede $2 \cdot 10 \cdot 20 + 10 \cdot 50 \cdot 20 = 10400$ moltiplicazioni, mentre calcolare il prodotto $(B \cdot C) \cdot D$ richiede $2 \cdot 10 \cdot 50 + 2 \cdot 50 \cdot 20 = 3000$ moltiplicazioni.


\vspace{1\baselineskip}
\noindent
Il problema del \textbf{Matrix Chain-Product} consiste nel determinare l'ordine delle moltiplicazioni tra matrici (tramite inserimento di opportune parentesi) che minimizza il numero totale di moltiplicazioni scalari necessarie per calcolare il prodotto finale.

\clearpage
\subsection*{Soluzione Dynamic Programming}
Per risolvere il problema del Matrix Chain-Product, definiamo i sotto-problemi come segue:
\begin{itemize}
    \item Consideriamo una catena di matrici da $A_i \times A_{i+1} \times \cdots \times A_j$, con $0 \leq i \leq j \leq n-1$.
    \item Definiamo $N(i,j)$ come il numero minimo di moltiplicazioni scalari necessarie per calcolare questo sotto-problema.
    \begin{itemize}[nosep]
        \item La soluzione al problema originale richiederà $N(0,n-1)$ moltiplicazioni scalari.
    \end{itemize}
\end{itemize}

\noindent
L'idea di base è che per qualsiasi ordine di moltiplicazione possibile, esiste un punto in cui la catena viene divisa in due sottocatene, ovvero il punto in cui verrà eseguito il \textbf{prodotto finale} tra due matrici risultanti. Supponiamo che questo prodotto finale si trovi all'indice $i$, per cui avremo che:

$$
\text{i:} \quad (A_0 \times \cdots \times A_i) \times (A_{i+1} \times \cdots \times A_{n-1})
$$

\noindent
Il costo della soluzione ottimale $N(0,n-1)$ è la somma del costo di due sotto-problemi ottimali $N(0,i)$ e $N(i+1,n-1)$, più il costo del prodotto finale tra le due matrici risultanti.

Quindi esistono diversi indici in cui possiamo dividere la catenza, e ogni scelta ha un costo differente. Possiamo calcolare $N(i,j)$ valutando ogni possibile punto di divisione $k$ tra $i$ e $j-1$, e scegliendo quello che minimizza il costo totale, per cui l'\textbf{equazione caratteristica} per $N(i,j)$ diventa:
$$
N(i,j) = \min\limits_{i \leq k < j} \{ N(i,k) + N(k+1,j) + d_i \cdot d_{k+1} \cdot d_{j+1} \}
$$

dove $d_i \cdot d_{k+1} \cdot d_{j+1}$ rappresenta il costo del prodotto finale tra le due matrici risultanti dalle sottocatene.

Possiamo, tuttavia, usare l'equazione per $N(i,j)$ per derivare un algoritmo efficiente calcolando i valori di $N(i,j)$ in modo bottom-up, memorizzando le soluzioni intermedie in una tabella di valori $N(i,j)$. Possiamo iniziare assegnando $N(i,i) = 0$, che rappresenta il caso base, per $i = 0,1,\ldots,n-1$. Possiamo quindi applicare l'equazione generale per calcolare i valori $N(i,i+1)$, poiché dipendono solo dai valori $N(i,i)$ e $N(i+1,i+1)$ che sono già disponibili. Dati i valori $N(i,i+1)$, possiamo quindi calcolare i valori $N(i,i+2)$, e così via. Pertanto, possiamo costruire i valori $N(i,j)$ a partire dai valori precedentemente calcolati fino a poter finalmente calcolare il valore di $N(0,n-1)$, che è il numero che stiamo cercando.

\vspace{2\baselineskip}
\noindent
L'algoritmo riportato di seguito calcola la tabella $N$ in tempo $O(n^3)$, poiché ci sono $O(n^2)$ celle nella tabella e il calcolo di ciascuna cella richiede tempo $O(n)$ per valutare tutti i possibili punti di divisione $k$. Lo spazio richiesto è $O(n^2)$ per memorizzare la tabella $N$.
Per ricostruire l'ordine ottimale delle moltiplicazioni, è necessario salvare per ogni cella l'indice del punto di divisione scelto per il calcolo del costo minimo per quella stessa cella.

\clearpage
\begin{lstlisting}
def matrix_chain(d):
    """
    d is a list of n+1 numbers such that size of kth matrix is d[k]-by-d[k+1].
    Return an n-by-n table such that N[i][j] represents the minimum number of multiplications needed to compute the product of Ai through Aj inclusive. """
    n = len(d) - 1                  # number of matrices
    N = [[0] * n for i in range(n)] # initialize n-by-n result to zero
    
    for b in range(1, n):           # number of products in subchain
        for i in range(n - b):      # start of subchain
            j = i + b               # end of subchain
            N[i][j] = float('inf')  # initialize to infinity
            
            # Find the split point k that minimizes cost
            for k in range(i, j):
                # Cost q = cost(left) + cost(right) + cost(multiplication)
                q = N[i][k] + N[k+1][j] + d[i] * d[k+1] * d[j+1]
                if q < N[i][j]:
                    N[i][j] = q
                    
    return N
\end{lstlisting}

\vspace{2\baselineskip}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.42\textwidth]{immagini/Dynamic/matrix_chain_table.png}
    \caption{La matrice triangolare superiore mostra il processo di calcolo: la diagonale principale (blu scuro) rappresenta i casi base di costo zero. Una cella specifica $(i, j)$ è evidenziata in azzurro, indicando il sottoproblema corrente. Le celle beige lungo la riga $i$ e la colonna $j$ evidenziano i sottoproblemi precedentemente risolti necessari per calcolare il valore corrente. La cella rossa nell'angolo in alto a destra è $N(0, n-1)$, e indica il risultato finale dell'algoritmo.}
    \label{fig:matrix_chain_table}
\end{figure}







